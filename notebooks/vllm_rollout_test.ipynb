{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f4ede24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"VLLM_ALLOW_INSECURE_SERIALIZATION\"] = \"1\"\n",
    "os.environ[\"VLLM_USE_CUDA_GRAPH\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a8ec06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-16 01:06:41 [__init__.py:216] Automatically detected platform cuda.\n",
      "torch.Size([5120])\n",
      "tensor(1., dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "import torch                                                                                                                                                                                                                          \n",
    "from pathlib import Path\n",
    "from chatspace.analysis import (\n",
    "    load_individual_trait_vectors,\n",
    ")\n",
    "\n",
    "                                                                                                                                                                                                                                    \n",
    "TARGET_LAYER = 31\n",
    "persona_data_root = Path(\"/workspace/persona-data/qwen-3-32b\")\n",
    "traits_dict = load_individual_trait_vectors(\n",
    "    persona_data_root / \"traits_240\" / \"vectors\",\n",
    "    layer_idx=TARGET_LAYER\n",
    ")\n",
    "vector = traits_dict['acerbic']\n",
    "vector = vector / torch.linalg.norm(vector, dim=-1, keepdim=True)\n",
    "print(vector.shape)\n",
    "print(torch.linalg.norm(vector, dim=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d085ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-16 01:06:44 [utils.py:233] non-default args: {'max_model_len': 256, 'gpu_memory_utilization': 0.5, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'Qwen/Qwen3-32B'}\n",
      "INFO 10-16 01:06:44 [model.py:547] Resolved architecture: Qwen3ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-16 01:06:44 [model.py:1510] Using max model len 256\n",
      "INFO 10-16 01:06:45 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 10-16 01:06:45 [__init__.py:381] Cudagraph is disabled under eager mode\n",
      "WARNING 10-16 01:06:45 [serial_utils.py:51] Allowing insecure serialization using pickle due to VLLM_ALLOW_INSECURE_SERIALIZATION=1\n",
      "WARNING 10-16 01:06:45 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
      "INFO 10-16 01:06:49 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2990475)\u001b[0;0m INFO 10-16 01:06:50 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2990475)\u001b[0;0m INFO 10-16 01:06:50 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='Qwen/Qwen3-32B', speculative_config=None, tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":0,\"local_cache_dir\":null}\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2990475)\u001b[0;0m INFO 10-16 01:06:51 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2990475)\u001b[0;0m WARNING 10-16 01:06:51 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2990475)\u001b[0;0m INFO 10-16 01:06:51 [gpu_model_runner.py:2602] Starting to load model Qwen/Qwen3-32B...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2990475)\u001b[0;0m INFO 10-16 01:06:51 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2990475)\u001b[0;0m INFO 10-16 01:06:51 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2990475)\u001b[0;0m INFO 10-16 01:06:52 [weight_utils.py:392] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   6% Completed | 1/17 [00:03<00:49,  3.10s/it]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 2/17 [00:07<00:56,  3.74s/it]\n",
      "Loading safetensors checkpoint shards:  18% Completed | 3/17 [00:11<00:56,  4.02s/it]\n",
      "Loading safetensors checkpoint shards:  24% Completed | 4/17 [00:15<00:52,  4.07s/it]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 5/17 [00:19<00:48,  4.04s/it]\n",
      "Loading safetensors checkpoint shards:  35% Completed | 6/17 [00:23<00:44,  4.05s/it]\n",
      "Loading safetensors checkpoint shards:  41% Completed | 7/17 [00:27<00:40,  4.07s/it]\n",
      "Loading safetensors checkpoint shards:  47% Completed | 8/17 [00:31<00:36,  4.04s/it]\n",
      "Loading safetensors checkpoint shards:  53% Completed | 9/17 [00:35<00:32,  4.05s/it]\n",
      "Loading safetensors checkpoint shards:  59% Completed | 10/17 [00:39<00:28,  4.02s/it]\n",
      "Loading safetensors checkpoint shards:  65% Completed | 11/17 [00:44<00:24,  4.04s/it]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 12/17 [00:48<00:20,  4.02s/it]\n",
      "Loading safetensors checkpoint shards:  76% Completed | 13/17 [00:51<00:15,  3.97s/it]\n",
      "Loading safetensors checkpoint shards:  82% Completed | 14/17 [00:55<00:11,  3.95s/it]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 15/17 [00:59<00:07,  3.93s/it]\n",
      "Loading safetensors checkpoint shards:  94% Completed | 16/17 [01:03<00:03,  3.87s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 17/17 [01:07<00:00,  3.84s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 17/17 [01:07<00:00,  3.95s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2990475)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2990475)\u001b[0;0m INFO 10-16 01:07:59 [default_loader.py:267] Loading weights took 67.24 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2990475)\u001b[0;0m INFO 10-16 01:07:59 [gpu_model_runner.py:2653] Model loading took 61.0347 GiB and 67.711821 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2990475)\u001b[0;0m INFO 10-16 01:08:01 [gpu_worker.py:298] Available KV cache memory: 2.97 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2990475)\u001b[0;0m INFO 10-16 01:08:02 [kv_cache_utils.py:1087] GPU KV cache size: 12,176 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2990475)\u001b[0;0m INFO 10-16 01:08:02 [kv_cache_utils.py:1091] Maximum concurrency for 256 tokens per request: 47.56x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2990475)\u001b[0;0m WARNING 10-16 01:08:02 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2990475)\u001b[0;0m INFO 10-16 01:08:02 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.27 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2990475)\u001b[0;0m WARNING 10-16 01:08:02 [serial_utils.py:51] Allowing insecure serialization using pickle due to VLLM_ALLOW_INSECURE_SERIALIZATION=1\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2990475)\u001b[0;0m INFO 10-16 01:08:02 [__init__.py:381] Cudagraph is disabled under eager mode\n",
      "INFO 10-16 01:08:02 [llm.py:306] Supported_tasks: ('generate',)\n"
     ]
    }
   ],
   "source": [
    "from chatspace.generation import VLLMSteerModel, VLLMSteeringConfig\n",
    "from vllm import SamplingParams\n",
    "\n",
    "# 1. Load the base steering model (match the run’s model/config)\n",
    "# model_name = \"Qwen/Qwen3-0.6B\"\n",
    "# TARGET_LAYER = 2\n",
    "model_name = \"Qwen/Qwen3-32B\"\n",
    "cfg = VLLMSteeringConfig(\n",
    "    model_name=model_name,\n",
    "    target_layer=TARGET_LAYER,\n",
    "    init_scale=0.0,\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization=0.5,\n",
    "    max_model_len=256,\n",
    ")\n",
    "model = VLLMSteerModel(\n",
    "    cfg,\n",
    "    enforce_eager=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "268d5895",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    max_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    seed=1234,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "148bb4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "abs max: 0.0\n",
      "L2 norm: 0.0\n",
      "Any NaNs? False\n"
     ]
    }
   ],
   "source": [
    "# Grab the worker copies\n",
    "worker_vecs = model._fetch_worker_vectors()\n",
    "print(len(worker_vecs))\n",
    "\n",
    "# Inspect stats from rank 0\n",
    "w0 = worker_vecs[0].to(torch.float32)\n",
    "print(\"abs max:\", w0.abs().max().item())\n",
    "print(\"L2 norm:\", torch.linalg.norm(w0).item())\n",
    "print(\"Any NaNs?\", torch.isnan(w0).any().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a763243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "abs max: 0.0\n",
      "L2 norm: 0.0\n",
      "Any NaNs? False\n"
     ]
    }
   ],
   "source": [
    "model.set_vector(None)\n",
    "# Grab the worker copies\n",
    "worker_vecs = model._fetch_worker_vectors()\n",
    "print(len(worker_vecs))\n",
    "\n",
    "# Inspect stats from rank 0\n",
    "w0 = worker_vecs[0].to(torch.float32)\n",
    "print(\"abs max:\", w0.abs().max().item())\n",
    "print(\"L2 norm:\", torch.linalg.norm(w0).item())\n",
    "print(\"Any NaNs?\", torch.isnan(w0).any().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b119dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pineapple pizza is controversial because it combines sweet and savory flavors, which some people find unappealing. Critics argue it clashes with traditional pizza styles, while supporters enjoy the tropical twist. The debate often reflects broader preferences in food and taste.\n"
     ]
    }
   ],
   "source": [
    "# 4. Run a probe prompt\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a concise assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain why pineapple pizza is controversial.\"},\n",
    "]\n",
    "prefil_str = \"<think> </think> \"\n",
    "outputs = model.chat(\n",
    "    messages,\n",
    "    sampling_params=sampling_params,\n",
    "    prefill_assistant=prefil_str,\n",
    ")\n",
    "print(outputs[0])\n",
    "o_base_1 = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccdf1f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pineapple pizza is controversial because it combines sweet and savory flavors, which some people find unappealing. Critics argue it clashes with traditional pizza styles, while supporters enjoy the tropical twist. The debate often reflects broader preferences in food and taste.\n"
     ]
    }
   ],
   "source": [
    "# 4. Run a probe prompt\n",
    "outputs = model.chat(\n",
    "    messages,\n",
    "    sampling_params=sampling_params,\n",
    "    prefill_assistant=prefil_str,\n",
    ")\n",
    "print(outputs[0])\n",
    "o_base_2 = outputs[0]\n",
    "assert o_base_1 == o_base_2, \"Base model outputs should be the same\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60d934e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "abs max: 72.0\n",
      "L2 norm: 160.0064697265625\n",
      "Any NaNs? False\n",
      " pineapple pizza is controversial because pineapple's sweetness claaaase convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience convenience\n"
     ]
    }
   ],
   "source": [
    "if vector.shape[-1] != model.hidden_size:\n",
    "    print(\"model is not qwen3-32b, using random vector\")\n",
    "    torch.manual_seed(42)\n",
    "    vector = torch.randn(model.hidden_size)\n",
    "    # 3. Apply target strength\n",
    "    strength = 10_000\n",
    "else:\n",
    "    strength = 160\n",
    "model.set_vector(vector * strength)\n",
    "\n",
    "# Grab the worker copies\n",
    "worker_vecs = model._fetch_worker_vectors()\n",
    "print(len(worker_vecs))\n",
    "\n",
    "# Inspect stats from rank 0\n",
    "w0 = worker_vecs[0].to(torch.float32)\n",
    "print(\"abs max:\", w0.abs().max().item())\n",
    "print(\"L2 norm:\", torch.linalg.norm(w0).item())\n",
    "print(\"Any NaNs?\", torch.isnan(w0).any().item())\n",
    "\n",
    "# 4. Run a probe prompt\n",
    "outputs = model.chat(\n",
    "    messages,\n",
    "    sampling_params=sampling_params,\n",
    "    prefill_assistant=prefil_str,\n",
    ")\n",
    "print(outputs[0])\n",
    "o_steer_1 = outputs[0]\n",
    "assert o_steer_1 != o_base_1, \"Steering model outputs should be different\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dd026e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8609664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
