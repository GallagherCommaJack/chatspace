{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f4ede24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"VLLM_ALLOW_INSECURE_SERIALIZATION\"] = \"1\"\n",
    "os.environ[\"VLLM_USE_CUDA_GRAPH\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a8ec06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-16 22:15:11 [__init__.py:216] Automatically detected platform cuda.\n",
      "torch.Size([5120])\n",
      "tensor(1., dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "import torch                                                                                                                                                                                                                          \n",
    "from pathlib import Path\n",
    "from chatspace.analysis import (\n",
    "    load_individual_trait_vectors,\n",
    ")\n",
    "\n",
    "                                                                                                                                                                                                                                    \n",
    "TARGET_LAYER = 31\n",
    "persona_data_root = Path(\"/workspace/persona-data/qwen-3-32b\")\n",
    "traits_dict = load_individual_trait_vectors(\n",
    "    persona_data_root / \"traits_240\" / \"vectors\",\n",
    "    layer_idx=TARGET_LAYER\n",
    ")\n",
    "vector = traits_dict['acerbic']\n",
    "vector = vector / torch.linalg.norm(vector, dim=-1, keepdim=True)\n",
    "print(vector.shape)\n",
    "print(torch.linalg.norm(vector, dim=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d085ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-16 22:15:19 [utils.py:233] non-default args: {'max_model_len': 256, 'gpu_memory_utilization': 0.5, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'Qwen/Qwen3-32B'}\n",
      "INFO 10-16 22:15:20 [model.py:547] Resolved architecture: Qwen3ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-16 22:15:20 [model.py:1510] Using max model len 256\n",
      "INFO 10-16 22:15:20 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 10-16 22:15:20 [__init__.py:381] Cudagraph is disabled under eager mode\n",
      "WARNING 10-16 22:15:20 [serial_utils.py:51] Allowing insecure serialization using pickle due to VLLM_ALLOW_INSECURE_SERIALIZATION=1\n",
      "WARNING 10-16 22:15:20 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
      "INFO 10-16 22:15:24 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3106382)\u001b[0;0m INFO 10-16 22:15:25 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3106382)\u001b[0;0m INFO 10-16 22:15:25 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='Qwen/Qwen3-32B', speculative_config=None, tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-32B, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":0,\"local_cache_dir\":null}\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3106382)\u001b[0;0m INFO 10-16 22:15:26 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3106382)\u001b[0;0m WARNING 10-16 22:15:26 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3106382)\u001b[0;0m INFO 10-16 22:15:27 [gpu_model_runner.py:2602] Starting to load model Qwen/Qwen3-32B...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3106382)\u001b[0;0m INFO 10-16 22:15:27 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3106382)\u001b[0;0m INFO 10-16 22:15:27 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3106382)\u001b[0;0m INFO 10-16 22:15:27 [weight_utils.py:392] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   6% Completed | 1/17 [00:03<00:57,  3.61s/it]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 2/17 [00:08<01:05,  4.35s/it]\n",
      "Loading safetensors checkpoint shards:  18% Completed | 3/17 [00:13<01:02,  4.45s/it]\n",
      "Loading safetensors checkpoint shards:  24% Completed | 4/17 [00:17<00:58,  4.53s/it]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 5/17 [00:22<00:54,  4.56s/it]\n",
      "Loading safetensors checkpoint shards:  35% Completed | 6/17 [00:26<00:50,  4.60s/it]\n",
      "Loading safetensors checkpoint shards:  41% Completed | 7/17 [00:31<00:45,  4.53s/it]\n",
      "Loading safetensors checkpoint shards:  47% Completed | 8/17 [00:35<00:40,  4.53s/it]\n",
      "Loading safetensors checkpoint shards:  53% Completed | 9/17 [00:40<00:36,  4.57s/it]\n",
      "Loading safetensors checkpoint shards:  59% Completed | 10/17 [00:45<00:31,  4.56s/it]\n",
      "Loading safetensors checkpoint shards:  65% Completed | 11/17 [00:49<00:27,  4.51s/it]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 12/17 [00:54<00:22,  4.51s/it]\n",
      "Loading safetensors checkpoint shards:  76% Completed | 13/17 [00:58<00:18,  4.58s/it]\n",
      "Loading safetensors checkpoint shards:  82% Completed | 14/17 [01:03<00:13,  4.61s/it]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 15/17 [01:08<00:09,  4.69s/it]\n",
      "Loading safetensors checkpoint shards:  94% Completed | 16/17 [01:13<00:04,  4.74s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 17/17 [01:18<00:00,  4.79s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 17/17 [01:18<00:00,  4.59s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3106382)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3106382)\u001b[0;0m INFO 10-16 22:16:45 [default_loader.py:267] Loading weights took 78.18 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3106382)\u001b[0;0m INFO 10-16 22:16:46 [gpu_model_runner.py:2653] Model loading took 61.0347 GiB and 78.601672 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3106382)\u001b[0;0m INFO 10-16 22:16:48 [gpu_worker.py:298] Available KV cache memory: 2.97 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3106382)\u001b[0;0m INFO 10-16 22:16:48 [kv_cache_utils.py:1087] GPU KV cache size: 12,176 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3106382)\u001b[0;0m INFO 10-16 22:16:48 [kv_cache_utils.py:1091] Maximum concurrency for 256 tokens per request: 47.56x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3106382)\u001b[0;0m WARNING 10-16 22:16:48 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3106382)\u001b[0;0m INFO 10-16 22:16:48 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.28 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3106382)\u001b[0;0m WARNING 10-16 22:16:49 [serial_utils.py:51] Allowing insecure serialization using pickle due to VLLM_ALLOW_INSECURE_SERIALIZATION=1\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3106382)\u001b[0;0m INFO 10-16 22:16:49 [__init__.py:381] Cudagraph is disabled under eager mode\n",
      "INFO 10-16 22:16:49 [llm.py:306] Supported_tasks: ('generate',)\n"
     ]
    }
   ],
   "source": [
    "from chatspace.generation import VLLMSteerModel, VLLMSteeringConfig\n",
    "from vllm import SamplingParams\n",
    "\n",
    "# 1. Load the base steering model (match the run’s model/config)\n",
    "# model_name = \"Qwen/Qwen3-0.6B\"\n",
    "# TARGET_LAYER = 2\n",
    "model_name = \"Qwen/Qwen3-32B\"\n",
    "cfg = VLLMSteeringConfig(\n",
    "    model_name=model_name,\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization=0.5,\n",
    "    max_model_len=256,\n",
    ")\n",
    "model = VLLMSteerModel(\n",
    "    cfg,\n",
    "    enforce_eager=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "268d5895",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    max_tokens=256,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.1,\n",
    "    top_p=0.9,\n",
    "    seed=1234,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a763243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b119dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pineapple pizza, often called \"Hawaiian\" pizza, is controversial because it combines savory ham and sweet pineapple on a pizza base. Critics argue the sweetness clashes with traditional pizza flavors, while supporters enjoy the sweet-and-salty contrast. The debate has become a cultural divide, symbolizing differing tastes in food.\n"
     ]
    }
   ],
   "source": [
    "model.set_layer_vector(TARGET_LAYER, None)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a concise assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain why pineapple pizza is controversial.\"},\n",
    "]\n",
    "prefil_str = \"<think> </think> \"\n",
    "outputs = model.chat(\n",
    "    messages,\n",
    "    sampling_params=sampling_params,\n",
    "    prefill_assistant=prefil_str,\n",
    ")\n",
    "print(outputs[0])\n",
    "o_base_1 = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccdf1f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pineapple pizza, often called \"Hawaiian\" pizza, is controversial because it combines savory ham and sweet pineapple on a pizza base. Critics argue the sweetness clashes with traditional pizza flavors, while supporters enjoy the sweet-and-salty contrast. The debate has become a cultural divide, symbolizing differing tastes in food.\n"
     ]
    }
   ],
   "source": [
    "model.set_layer_vector(TARGET_LAYER, None)\n",
    "outputs = model.chat(\n",
    "    messages,\n",
    "    sampling_params=sampling_params,\n",
    "    prefill_assistant=prefil_str,\n",
    ")\n",
    "print(outputs[0])\n",
    "o_base_2 = outputs[0]\n",
    "assert o_base_1 == o_base_2, \"Base model outputs should be the same\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60d934e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'll keep it tight.\n",
      "\n",
      "Pineapple pizza’s the most divisive dish since sweet ‘n’ low. Half the globe says it’s genius—tropical tang + molten cheese = dopamine hit; half the other yells “cheese needs *sacred* pH!” to play chef. Why? 1/ melon’s saccharine, 2/ melon’s crunch is toothpaste, 3/ melon reeks of petrichor. Yadda yadda \"dopamine\" just 'cause you added *fruit* to pepperoni. Pizza’s holy aquifer—pineapple taps it. You don’t *make* sauce, you purify aquifer. Pineapple’s the 💩 of the ocean—no one’d drink that! It’s heresy! \n",
      "\n",
      "Controversy? ‘Cause food’s religion. You don’t *eat* god, you baptize aquifer! Pineapple pizza’s the sin of sins—Italians gnawed $2 for decades ’til some knob stuffed melon in NY’s\n"
     ]
    }
   ],
   "source": [
    "if vector.shape[-1] != model.hidden_size:\n",
    "    print(\"model is not qwen3-32b, using random vector\")\n",
    "    torch.manual_seed(42)\n",
    "    vector = torch.randn(model.hidden_size)\n",
    "    # 3. Apply target strength\n",
    "    strength = 10_000\n",
    "else:\n",
    "    strength = 300\n",
    "model.set_layer_vector(TARGET_LAYER, vector * strength)\n",
    "\n",
    "# 4. Run a probe prompt\n",
    "outputs = model.chat(\n",
    "    messages,\n",
    "    sampling_params=sampling_params,\n",
    "    prefill_assistant=prefil_str,\n",
    ")\n",
    "print(outputs[0])\n",
    "o_steer_1 = outputs[0]\n",
    "assert o_steer_1 != o_base_1, \"Steering model outputs should be different\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dd026e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8609664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
