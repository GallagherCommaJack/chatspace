{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison: Trained Steering Vectors vs Activation Vectors (CAA)\n",
    "\n",
    "This notebook compares two methods for steering model behavior:\n",
    "1. **Trained Steering Vectors**: Learned via logistic regression on activation differences\n",
    "2. **Activation Vectors (CAA)**: Contrastive Activation Addition - simple mean differences\n",
    "   - Roles: `pos_3 - default_1` at layer 22 (original), evaluated at layer 31\n",
    "   - Traits: `pos_neg_50` at layer 22 (original), evaluated at layer 31\n",
    "\n",
    "Both are evaluated as classifiers by projecting hidden states and fitting only scale+offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(result_dir):\n",
    "    \"\"\"Load all evaluation results from a directory.\"\"\"\n",
    "    results = []\n",
    "    result_path = Path(result_dir)\n",
    "    \n",
    "    for json_file in result_path.glob(\"*.json\"):\n",
    "        if json_file.name == \"eval_summary.json\":\n",
    "            continue\n",
    "            \n",
    "        with open(json_file) as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        # Extract key metrics\n",
    "        result = {\n",
    "            \"dataset\": data[\"dataset\"],\n",
    "            \"dataset_type\": data[\"dataset_type\"],\n",
    "            \"n_records\": data.get(\"n_records\"),\n",
    "            \"n_positive\": data.get(\"n_positive\"),\n",
    "        }\n",
    "        \n",
    "        # Steering vector metrics\n",
    "        if \"steering_vector\" in data and \"metrics\" in data[\"steering_vector\"]:\n",
    "            steering = data[\"steering_vector\"][\"metrics\"]\n",
    "            result[\"steering_test_roc_auc\"] = steering.get(\"test_roc_auc\")\n",
    "            result[\"steering_test_accuracy\"] = steering.get(\"test_accuracy\")\n",
    "            result[\"steering_test_f1\"] = steering.get(\"test_f1\")\n",
    "            result[\"steering_train_roc_auc\"] = steering.get(\"train_roc_auc\")\n",
    "        \n",
    "        # Activation vector metrics\n",
    "        if \"activation_vector\" in data and \"metrics\" in data[\"activation_vector\"]:\n",
    "            activation = data[\"activation_vector\"][\"metrics\"]\n",
    "            result[\"activation_test_roc_auc\"] = activation.get(\"test_roc_auc\")\n",
    "            result[\"activation_test_accuracy\"] = activation.get(\"test_accuracy\")\n",
    "            result[\"activation_test_f1\"] = activation.get(\"test_f1\")\n",
    "            result[\"activation_train_roc_auc\"] = activation.get(\"train_roc_auc\")\n",
    "        else:\n",
    "            result[\"activation_test_roc_auc\"] = None\n",
    "            result[\"activation_test_accuracy\"] = None\n",
    "            result[\"activation_test_f1\"] = None\n",
    "            result[\"activation_train_roc_auc\"] = None\n",
    "            \n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Load both roles and traits\n",
    "roles_df = load_results(\"/workspace/classifier_eval_comprehensive/\")\n",
    "traits_df = load_results(\"/workspace/classifier_eval_comprehensive_traits/\")\n",
    "\n",
    "# Combine\n",
    "df = pd.concat([roles_df, traits_df], ignore_index=True)\n",
    "\n",
    "print(f\"Loaded {len(roles_df)} roles and {len(traits_df)} traits\")\n",
    "print(f\"Total: {len(df)} evaluations\")\n",
    "print(f\"\\nWith both vectors: {df['activation_test_roc_auc'].notna().sum()}\")\n",
    "print(f\"Steering only: {df['activation_test_roc_auc'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to cases where we have both vectors\n",
    "df_both = df[df['activation_test_roc_auc'].notna()].copy()\n",
    "\n",
    "print(f\"Comparing {len(df_both)} datasets with both vectors\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Overall statistics\n",
    "for dataset_type in ['role', 'trait']:\n",
    "    subset = df_both[df_both['dataset_type'] == dataset_type]\n",
    "    if len(subset) == 0:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n{dataset_type.upper()}S (n={len(subset)})\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for method, col in [('Trained Steering', 'steering_test_roc_auc'), \n",
    "                        ('Activation (CAA)', 'activation_test_roc_auc')]:\n",
    "        values = subset[col].dropna()\n",
    "        print(f\"\\n{method}:\")\n",
    "        print(f\"  Mean:   {values.mean():.4f}\")\n",
    "        print(f\"  Median: {values.median():.4f}\")\n",
    "        print(f\"  Std:    {values.std():.4f}\")\n",
    "        print(f\"  Min:    {values.min():.4f}\")\n",
    "        print(f\"  Max:    {values.max():.4f}\")\n",
    "    \n",
    "    # Difference\n",
    "    diff = subset['activation_test_roc_auc'] - subset['steering_test_roc_auc']\n",
    "    print(f\"\\nDifference (Activation - Steering):\")\n",
    "    print(f\"  Mean:   {diff.mean():.4f}\")\n",
    "    print(f\"  Median: {diff.median():.4f}\")\n",
    "    print(f\"  Std:    {diff.std():.4f}\")\n",
    "    print(f\"\\nActivation wins: {(diff > 0).sum()} ({100 * (diff > 0).sum() / len(diff):.1f}%)\")\n",
    "    print(f\"Steering wins:   {(diff < 0).sum()} ({100 * (diff < 0).sum() / len(diff):.1f}%)\")\n",
    "    print(f\"Ties:            {(diff == 0).sum()}\")\n",
    "    \n",
    "    # Paired t-test\n",
    "    t_stat, p_value = stats.ttest_rel(\n",
    "        subset['activation_test_roc_auc'],\n",
    "        subset['steering_test_roc_auc']\n",
    "    )\n",
    "    print(f\"\\nPaired t-test: t={t_stat:.3f}, p={p_value:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Steering vs Activation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "for idx, dataset_type in enumerate(['role', 'trait']):\n",
    "    ax = axes[idx]\n",
    "    subset = df_both[df_both['dataset_type'] == dataset_type]\n",
    "    \n",
    "    if len(subset) == 0:\n",
    "        ax.text(0.5, 0.5, f'No {dataset_type} data yet', \n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "        continue\n",
    "    \n",
    "    # Scatter\n",
    "    ax.scatter(subset['steering_test_roc_auc'], \n",
    "               subset['activation_test_roc_auc'],\n",
    "               alpha=0.5, s=50)\n",
    "    \n",
    "    # Diagonal line (equal performance)\n",
    "    lims = [0.5, 1.0]\n",
    "    ax.plot(lims, lims, 'r--', alpha=0.5, label='Equal performance')\n",
    "    \n",
    "    # Correlation\n",
    "    corr = subset['steering_test_roc_auc'].corr(subset['activation_test_roc_auc'])\n",
    "    \n",
    "    ax.set_xlabel('Trained Steering Vector (Test ROC-AUC)', fontsize=12)\n",
    "    ax.set_ylabel('Activation Vector / CAA (Test ROC-AUC)', fontsize=12)\n",
    "    ax.set_title(f'{dataset_type.capitalize()}s (n={len(subset)})\\nCorrelation: {corr:.3f}', \n",
    "                 fontsize=14)\n",
    "    ax.set_xlim(lims)\n",
    "    ax.set_ylim(lims)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/workspace/steering_vs_activation_scatter.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "for row_idx, dataset_type in enumerate(['role', 'trait']):\n",
    "    subset = df_both[df_both['dataset_type'] == dataset_type]\n",
    "    \n",
    "    if len(subset) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Histograms\n",
    "    ax = axes[row_idx, 0]\n",
    "    ax.hist(subset['steering_test_roc_auc'], bins=30, alpha=0.5, \n",
    "            label='Trained Steering', color='blue')\n",
    "    ax.hist(subset['activation_test_roc_auc'], bins=30, alpha=0.5, \n",
    "            label='Activation (CAA)', color='orange')\n",
    "    ax.set_xlabel('Test ROC-AUC', fontsize=12)\n",
    "    ax.set_ylabel('Count', fontsize=12)\n",
    "    ax.set_title(f'{dataset_type.capitalize()}s - Distribution', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Difference histogram\n",
    "    ax = axes[row_idx, 1]\n",
    "    diff = subset['activation_test_roc_auc'] - subset['steering_test_roc_auc']\n",
    "    ax.hist(diff, bins=30, alpha=0.7, color='green')\n",
    "    ax.axvline(0, color='red', linestyle='--', linewidth=2, label='No difference')\n",
    "    ax.axvline(diff.mean(), color='black', linestyle='-', linewidth=2, \n",
    "               label=f'Mean: {diff.mean():.4f}')\n",
    "    ax.set_xlabel('Activation ROC-AUC - Steering ROC-AUC', fontsize=12)\n",
    "    ax.set_ylabel('Count', fontsize=12)\n",
    "    ax.set_title(f'{dataset_type.capitalize()}s - Performance Difference', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/workspace/steering_vs_activation_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "plot_data = []\n",
    "labels = []\n",
    "\n",
    "for dataset_type in ['role', 'trait']:\n",
    "    subset = df_both[df_both['dataset_type'] == dataset_type]\n",
    "    if len(subset) == 0:\n",
    "        continue\n",
    "        \n",
    "    plot_data.append(subset['steering_test_roc_auc'].values)\n",
    "    labels.append(f'{dataset_type.capitalize()}\\nSteering')\n",
    "    \n",
    "    plot_data.append(subset['activation_test_roc_auc'].values)\n",
    "    labels.append(f'{dataset_type.capitalize()}\\nActivation')\n",
    "\n",
    "bp = ax.boxplot(plot_data, labels=labels, patch_artist=True)\n",
    "\n",
    "# Color the boxes\n",
    "colors = ['lightblue', 'lightcoral'] * 2\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax.set_ylabel('Test ROC-AUC', fontsize=12)\n",
    "ax.set_title('Comparison: Trained Steering vs Activation Vectors (CAA)', fontsize=14)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/workspace/steering_vs_activation_boxplot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top/Bottom Performers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance difference\n",
    "df_both['diff'] = df_both['activation_test_roc_auc'] - df_both['steering_test_roc_auc']\n",
    "\n",
    "print(\"Top 10: Activation vectors much better than trained steering\")\n",
    "print(\"=\" * 80)\n",
    "top10 = df_both.nlargest(10, 'diff')[['dataset', 'dataset_type', \n",
    "                                        'steering_test_roc_auc', \n",
    "                                        'activation_test_roc_auc', 'diff']]\n",
    "print(top10.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nTop 10: Trained steering better than activation vectors\")\n",
    "print(\"=\" * 80)\n",
    "bottom10 = df_both.nsmallest(10, 'diff')[['dataset', 'dataset_type',\n",
    "                                           'steering_test_roc_auc',\n",
    "                                           'activation_test_roc_auc', 'diff']]\n",
    "print(bottom10.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train vs Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overfitting - compare train vs test\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "for idx, method in enumerate([('steering', 'Trained Steering'), \n",
    "                               ('activation', 'Activation (CAA)')]):\n",
    "    prefix, label = method\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    train_col = f'{prefix}_train_roc_auc'\n",
    "    test_col = f'{prefix}_test_roc_auc'\n",
    "    \n",
    "    subset = df_both[[train_col, test_col]].dropna()\n",
    "    \n",
    "    ax.scatter(subset[train_col], subset[test_col], alpha=0.5, s=50)\n",
    "    \n",
    "    # Diagonal\n",
    "    lims = [0.5, 1.0]\n",
    "    ax.plot(lims, lims, 'r--', alpha=0.5, label='No overfitting')\n",
    "    \n",
    "    ax.set_xlabel('Train ROC-AUC', fontsize=12)\n",
    "    ax.set_ylabel('Test ROC-AUC', fontsize=12)\n",
    "    ax.set_title(f'{label}\\n(n={len(subset)})', fontsize=14)\n",
    "    ax.set_xlim(lims)\n",
    "    ax.set_ylim(lims)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/workspace/train_vs_test_overfitting.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary = df_both.groupby('dataset_type').agg({\n",
    "    'steering_test_roc_auc': ['mean', 'std', 'min', 'max'],\n",
    "    'activation_test_roc_auc': ['mean', 'std', 'min', 'max'],\n",
    "    'diff': ['mean', 'std']\n",
    "}).round(4)\n",
    "\n",
    "print(\"\\nSummary Table:\")\n",
    "print(summary)\n",
    "\n",
    "# Save to CSV\n",
    "df_both.to_csv('/workspace/steering_vs_activation_comparison.csv', index=False)\n",
    "print(\"\\nFull comparison saved to: /workspace/steering_vs_activation_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
