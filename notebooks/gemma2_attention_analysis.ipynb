{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma2-27B Attention Analysis\n",
    "\n",
    "This notebook analyzes how instruction tuning modifies attention patterns for PC and semantic vectors.\n",
    "\n",
    "**Key Analyses:**\n",
    "1. **QK Affinity**: Raw attention logits (before softmax) reveal semantic affinities\n",
    "2. **VO Decomposition**: What semantic content flows through when attending to a vector\n",
    "3. **Base vs Instruct Comparison**: How instruction tuning changes routing\n",
    "\n",
    "**Approach:**\n",
    "- Compute QK affinity matrices for PC and semantic vectors\n",
    "- Compute VO decomposition (value-output transformation)\n",
    "- Analyze z-scores relative to random baseline\n",
    "- Identify layers and patterns where instruction tuning has strongest effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "from collections import OrderedDict\n",
    "\n",
    "from chatspace.analysis import (\n",
    "    load_pca_data,\n",
    "    extract_pc_components,\n",
    "    load_layer_semantic_vectors,\n",
    "    normalize_vector,\n",
    "    compute_qk_affinity_matrix,\n",
    "    compute_vo_decomposition,\n",
    "    compute_z_score_matrices,\n",
    "    get_top_interactions,\n",
    "    analyze_pc_pattern\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models and Prepare Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "base_model_id = \"google/gemma-2-27b\"\n",
    "instruct_model_id = \"google/gemma-2-27b-it\"\n",
    "\n",
    "print(\"Loading models...\")\n",
    "config = AutoConfig.from_pretrained(base_model_id)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "instruct_model = AutoModelForCausalLM.from_pretrained(instruct_model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "print(\"\u2713 Models loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PC vectors\n",
    "persona_data_root = Path(\"/workspace/persona-data\")\n",
    "roles_pca_dir = persona_data_root / \"gemma-2-27b\" / \"roles_240\" / \"pca\"\n",
    "traits_pca_dir = persona_data_root / \"gemma-2-27b\" / \"traits_240\" / \"pca\"\n",
    "\n",
    "pca_data, _ = load_pca_data(roles_pca_dir)\n",
    "pcs, variance_explained = extract_pc_components(pca_data, n_components=3)\n",
    "\n",
    "# Build test vectors\n",
    "test_vectors = OrderedDict()\n",
    "test_vectors['PC1'] = normalize_vector(pcs[0])\n",
    "test_vectors['PC2'] = normalize_vector(pcs[1])\n",
    "test_vectors['PC3'] = normalize_vector(pcs[2])\n",
    "test_vectors['-PC1'] = normalize_vector(-pcs[0].float())\n",
    "\n",
    "# Add random baseline\n",
    "torch.manual_seed(42)\n",
    "for i in range(5):\n",
    "    rand_vec = torch.randn(config.hidden_size, dtype=torch.float32)\n",
    "    test_vectors[f'Random{i+1}'] = normalize_vector(rand_vec)\n",
    "\n",
    "vector_names = list(test_vectors.keys())\n",
    "vectors_tensor = torch.stack([test_vectors[name] for name in vector_names])\n",
    "\n",
    "print(f\"\u2713 Prepared {len(vector_names)} test vectors\")\n",
    "print(f\"  PC vectors: {[n for n in vector_names if 'PC' in n]}\")\n",
    "print(f\"  Random vectors: {[n for n in vector_names if 'Random' in n]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute QK Affinity Matrices\n\nQK affinities reveal which vectors semantically \"attend to\" each other (before softmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute QK affinities for target layers\n",
    "target_layers = [15, 18, 20, 22, 25]\n",
    "n_vectors = len(vector_names)\n",
    "\n",
    "qk_base = np.zeros((len(target_layers), n_vectors, n_vectors))\n",
    "qk_instruct = np.zeros((len(target_layers), n_vectors, n_vectors))\n",
    "\n",
    "print(f\"Computing QK affinities for layers {target_layers}...\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for i, layer_idx in enumerate(tqdm(target_layers)):\n",
    "        qk_b = compute_qk_affinity_matrix(vectors_tensor, layer_idx, base_model)\n",
    "        qk_i = compute_qk_affinity_matrix(vectors_tensor, layer_idx, instruct_model)\n",
    "        qk_base[i] = qk_b.cpu().numpy()\n",
    "        qk_instruct[i] = qk_i.cpu().numpy()\n",
    "\n",
    "qk_delta = qk_instruct - qk_base\n",
    "print(f\"\\n\u2713 QK affinity computation complete\")\n",
    "print(f\"  Shape: {qk_base.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute z-scores relative to random baseline\n",
    "random_indices = [vector_names.index(n) for n in vector_names if 'Random' in n]\n",
    "qk_delta_z = compute_z_score_matrices(qk_delta, random_indices)\n",
    "\n",
    "print(f\"\u2713 Z-score normalization complete\")\n",
    "print(f\"\\nExample z-scores at layer 18 (index 1):\")\n",
    "if 'PC1' in vector_names:\n",
    "    pc1_idx = vector_names.index('PC1')\n",
    "    print(f\"  PC1\u2192PC1:  {qk_delta_z[1, pc1_idx, pc1_idx]:.2f}\u03c3\")\n",
    "    print(f\"  PC1\u2192-PC1: {qk_delta_z[1, pc1_idx, vector_names.index('-PC1')]:.2f}\u03c3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize QK affinity changes\n",
    "fig, axes = plt.subplots(1, len(target_layers), figsize=(20, 4))\n",
    "\n",
    "for i, layer_idx in enumerate(target_layers):\n",
    "    # Show only PC vectors for clarity\n",
    "    pc_indices = [vector_names.index(n) for n in vector_names if 'PC' in n]\n",
    "    pc_names = [vector_names[j] for j in pc_indices]\n",
    "    \n",
    "    matrix = qk_delta_z[i][np.ix_(pc_indices, pc_indices)]\n",
    "    \n",
    "    sns.heatmap(matrix, ax=axes[i], cmap='RdBu_r', center=0, \n",
    "                xticklabels=pc_names, yticklabels=pc_names,\n",
    "                cbar_kws={'label': 'Z-score'})\n",
    "    axes[i].set_title(f'Layer {layer_idx}\\nQK Delta (Z-scores)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute VO Decomposition\n\nVO decomposition reveals what semantic content flows through when attending to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VO decomposition\n",
    "vo_base = np.zeros((len(target_layers), n_vectors, n_vectors))\n",
    "vo_instruct = np.zeros((len(target_layers), n_vectors, n_vectors))\n",
    "\n",
    "print(f\"Computing VO decomposition for layers {target_layers}...\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for i, layer_idx in enumerate(tqdm(target_layers)):\n",
    "        vo_b = compute_vo_decomposition(vectors_tensor, vectors_tensor, layer_idx, base_model)\n",
    "        vo_i = compute_vo_decomposition(vectors_tensor, vectors_tensor, layer_idx, instruct_model)\n",
    "        vo_base[i] = vo_b.cpu().numpy()\n",
    "        vo_instruct[i] = vo_i.cpu().numpy()\n",
    "\n",
    "vo_delta = vo_instruct - vo_base\n",
    "vo_delta_z = compute_z_score_matrices(vo_delta, random_indices)\n",
    "\n",
    "print(f\"\\n\u2713 VO decomposition complete\")\n",
    "print(f\"  Shape: {vo_base.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze top VO patterns\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP VO DECOMPOSITION CHANGES (Layer 18)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "layer_18_idx = target_layers.index(18)\n",
    "top_vo = get_top_interactions(vo_delta_z, vector_names, layer_18_idx, top_k=10, exclude_self=True)\n",
    "print(\"\\nTop 10 VO changes (what semantic content flows when attending):\")\n",
    "print(top_vo.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PC1-Specific Analysis\n\nFocus on how PC1 attention patterns change with instruction tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze PC1 patterns across layers\n",
    "if 'PC1' in vector_names:\n",
    "    pc1_qk = analyze_pc_pattern(qk_delta_z, vector_names, 'PC1', layer_18_idx, top_k=10)\n",
    "    pc1_vo = analyze_pc_pattern(vo_delta_z, vector_names, 'PC1', layer_18_idx, top_k=10)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PC1 ATTENTION PATTERNS (Layer 18)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nQK Affinity (what PC1 attends to):\")\n",
    "    print(pc1_qk.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nVO Decomposition (what flows through when attending to PC1):\")\n",
    "    print(pc1_vo.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n\nThis analysis reveals how instruction tuning modifies attention routing for PC and semantic vectors. Key findings:\n\n- **QK Affinity**: Shows which vectors semantically attend to each other\n- **VO Decomposition**: Reveals semantic content flow through attention\n- **Layer-specific effects**: Different layers show different attention modification patterns\n- **PC1 patterns**: PC1 shows distinctive attention changes, particularly at layer 18\n\nThese complement the MLP analysis by revealing how semantic information is **routed** (attention) vs **transformed** (MLP)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}