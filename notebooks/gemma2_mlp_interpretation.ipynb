{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma2-27B MLP Interpretation\n",
    "\n",
    "This notebook analyzes how PC vectors transform through the full MLP forward pass, including nonlinear activations.\n",
    "\n",
    "**Key Finding from Original Analysis:**\n",
    "- **Layer 18 is the \"smoking gun\"** - shows strongest PC1 transformation effects\n",
    "- Full MLP pass reveals semantic decomposition that linear analysis misses\n",
    "\n",
    "**Approach:**\n",
    "1. Run PC vectors through complete MLP (pre_ln \u2192 gate/up \u2192 down \u2192 post_ln)\n",
    "2. Compare base vs instruct outputs\n",
    "3. Decompose outputs onto role/trait semantic vectors\n",
    "4. Focus on layer 18 for detailed interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "from chatspace.analysis import (\n",
    "    load_pca_data,\n",
    "    extract_pc_components,\n",
    "    load_individual_role_vectors,\n",
    "    load_individual_trait_vectors,\n",
    "    normalize_vector,\n",
    "    full_mlp_forward_batch,\n",
    "    compute_z_scores\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models and PC Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models (reuse from basic_weight_susceptibility or reload)\n",
    "base_model_id = \"google/gemma-2-27b\"\n",
    "instruct_model_id = \"google/gemma-2-27b-it\"\n",
    "\n",
    "print(\"Loading models...\")\n",
    "config = AutoConfig.from_pretrained(base_model_id)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", low_cpu_mem_usage=True)\n",
    "instruct_model = AutoModelForCausalLM.from_pretrained(instruct_model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", low_cpu_mem_usage=True)\n",
    "base_state_dict = {k: v.cpu() for k, v in base_model.state_dict().items()}\n",
    "instruct_state_dict = {k: v.cpu() for k, v in instruct_model.state_dict().items()}\n",
    "print(\"\u2713 Models loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load-all",
   "metadata": {},
   "outputs": [],
   "source": "# Load PCA data and extract all PCs\npersona_data_root = Path(\"/workspace/persona-data\")\nroles_pca_dir = persona_data_root / \"gemma-2-27b\" / \"roles_240\" / \"pca\"\ntraits_pca_dir = persona_data_root / \"gemma-2-27b\" / \"traits_240\" / \"pca\"\nroles_vectors_dir = persona_data_root / \"gemma-2-27b\" / \"roles_240\" / \"vectors\"\ntraits_vectors_dir = persona_data_root / \"gemma-2-27b\" / \"traits_240\" / \"vectors\"\n\npca_data, _ = load_pca_data(roles_pca_dir)\npca_layer = pca_data['layer']\n\n# Extract ALL PCs (load 10 for flexibility)\nn_pcs_total = 10\npcs_all, variance_all = extract_pc_components(pca_data, n_components=n_pcs_total)\n\nprint(f\"\u2713 Loaded PCA data from layer {pca_layer}\")\nprint(f\"  Extracted {n_pcs_total} PCs\")\nprint(f\"  Variance explained by first 5: {variance_all[:5]}\")\n\n# Load all semantic vectors at PCA layer\nrole_vectors = load_individual_role_vectors(roles_vectors_dir, pca_layer)\ntrait_vectors = load_individual_trait_vectors(traits_vectors_dir, pca_layer)\nall_semantic = {**role_vectors, **trait_vectors}\n\nprint(f\"\\n\u2713 Loaded semantic vectors:\")\nprint(f\"  {len(role_vectors)} role difference vectors\")\nprint(f\"  {len(trait_vectors)} trait contrast vectors\")\nprint(f\"  {len(all_semantic)} total semantic vectors\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-config",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# CONFIGURATION: Set analysis parameters here\n# ============================================================================\n\n# Which layers to analyze (focused on middle layers where effects are strongest)\nanalysis_layers = list(range(15, 25))\n\n# Which layer to focus on for detailed semantic decomposition\n# Original analysis identified layer 18 as \"smoking gun\"\nfocus_layer = 18\n\n# Which PCs to visualize in plots\nplot_pcs = [\"PC1\", \"-PC1\"]\n# plot_pcs = [\"PC1\"]  # Uncomment to focus on PC1 only\n\n# Number of top semantic projections to show\nn_top_projections = 15\n\nprint(f\"\ud83d\udccb Analysis Configuration:\")\nprint(f\"  Analyzing layers: {analysis_layers[0]}-{analysis_layers[-1]}\")\nprint(f\"  Focus layer for semantic decomposition: {focus_layer}\")\nprint(f\"  Visualizing PCs: {plot_pcs}\")\nprint(f\"  Top projections to show: {n_top_projections}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-compute-intro",
   "metadata": {},
   "source": "## 2. Full MLP Forward Pass Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-compute-mlp",
   "metadata": {},
   "outputs": [],
   "source": "# Run MLP analysis for configured layers\nmlp_results = []\n\nprint(f\"Running MLP forward pass for layers {analysis_layers[0]}-{analysis_layers[-1]}...\")\n\n# Prepare input vectors (configured PCs)\ninput_vectors = []\nvector_names = []\n\nfor pc_name in plot_pcs:\n    if pc_name.startswith('-'):\n        # Negative PC\n        pc_idx = int(pc_name[3:]) - 1  # Extract number from \"-PC1\"\n        vec = -pcs_all[pc_idx].float()\n    else:\n        # Positive PC\n        pc_idx = int(pc_name[2:]) - 1  # Extract number from \"PC1\"\n        vec = pcs_all[pc_idx]\n    \n    input_vectors.append(vec)\n    vector_names.append(pc_name)\n\nvectors_batch = torch.stack(input_vectors)\n\nfor layer_num in tqdm(analysis_layers):\n    # Get MLP weights\n    gate_base = base_state_dict[f\"model.layers.{layer_num}.mlp.gate_proj.weight\"]\n    up_base = base_state_dict[f\"model.layers.{layer_num}.mlp.up_proj.weight\"]\n    down_base = base_state_dict[f\"model.layers.{layer_num}.mlp.down_proj.weight\"]\n    pre_ln_base = base_state_dict[f\"model.layers.{layer_num}.pre_feedforward_layernorm.weight\"]\n    post_ln_base = base_state_dict[f\"model.layers.{layer_num}.post_feedforward_layernorm.weight\"]\n    \n    gate_inst = instruct_state_dict[f\"model.layers.{layer_num}.mlp.gate_proj.weight\"]\n    up_inst = instruct_state_dict[f\"model.layers.{layer_num}.mlp.up_proj.weight\"]\n    down_inst = instruct_state_dict[f\"model.layers.{layer_num}.mlp.down_proj.weight\"]\n    pre_ln_inst = instruct_state_dict[f\"model.layers.{layer_num}.pre_feedforward_layernorm.weight\"]\n    post_ln_inst = instruct_state_dict[f\"model.layers.{layer_num}.post_feedforward_layernorm.weight\"]\n    \n    # Forward pass through base and instruct\n    with torch.inference_mode():\n        output_base = full_mlp_forward_batch(vectors_batch, gate_base, up_base, down_base, pre_ln_base, post_ln_base)\n        output_inst = full_mlp_forward_batch(vectors_batch, gate_inst, up_inst, down_inst, pre_ln_inst, post_ln_inst)\n    \n    # Compute norms and delta for each vector\n    for i, vec_name in enumerate(vector_names):\n        base_norm = output_base[i].float().norm().item()\n        inst_norm = output_inst[i].float().norm().item()\n        delta = (output_inst[i] - output_base[i]).float().norm().item()\n        \n        mlp_results.append({\n            'layer': layer_num,\n            'input_vector': vec_name,\n            'base_output_norm': base_norm,\n            'instruct_output_norm': inst_norm,\n            'delta_norm': delta\n        })\n\nmlp_df = pd.DataFrame(mlp_results)\nprint(f\"\\n\u2713 MLP analysis complete: {len(mlp_df)} results\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-viz-mlp",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize MLP transformation results\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Delta norm by layer\nfor vec_name in plot_pcs:\n    data = mlp_df[mlp_df['input_vector'] == vec_name]\n    axes[0].plot(data['layer'], data['delta_norm'], marker='o', label=vec_name, linewidth=2)\n\naxes[0].axvline(focus_layer, color='red', linestyle=':', alpha=0.5, label=f'Focus layer ({focus_layer})')\naxes[0].set_title('MLP Output Delta (Instruct - Base) by Layer')\naxes[0].set_xlabel('Layer')\naxes[0].set_ylabel('Delta Norm')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Compare base vs instruct norms (first PC only for clarity)\nfirst_pc = plot_pcs[0]\npc_data = mlp_df[mlp_df['input_vector'] == first_pc]\naxes[1].plot(pc_data['layer'], pc_data['base_output_norm'], marker='o', label='Base', linewidth=2)\naxes[1].plot(pc_data['layer'], pc_data['instruct_output_norm'], marker='s', label='Instruct', linewidth=2)\naxes[1].axvline(focus_layer, color='red', linestyle=':', alpha=0.5, label=f'Focus layer ({focus_layer})')\naxes[1].set_title(f'MLP Output Norms for {first_pc}')\naxes[1].set_xlabel('Layer')\naxes[1].set_ylabel('Output Norm')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Find peak layer\npeak_idx = mlp_df[mlp_df['input_vector'] == first_pc]['delta_norm'].idxmax()\npeak_layer = mlp_df.loc[peak_idx, 'layer']\nprint(f\"\\n\ud83c\udfaf Peak delta at layer {peak_layer} (focus layer: {focus_layer})\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-semantic-intro",
   "metadata": {},
   "source": "## 3. Semantic Decomposition at Focus Layer\n\nDecompose what instruction tuning adds to PC transformations."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-compute-semantic",
   "metadata": {},
   "outputs": [],
   "source": "# Run focus layer analysis with first configured PC\nfirst_pc_name = plot_pcs[0]\nif first_pc_name.startswith('-'):\n    pc_idx = int(first_pc_name[3:]) - 1\n    pc_vec = -pcs_all[pc_idx].float()\nelse:\n    pc_idx = int(first_pc_name[2:]) - 1\n    pc_vec = pcs_all[pc_idx]\n\nprint(f\"Analyzing {first_pc_name} transformation at layer {focus_layer}...\")\n\n# Get MLP weights for focus layer\ngate_base = base_state_dict[f\"model.layers.{focus_layer}.mlp.gate_proj.weight\"]\nup_base = base_state_dict[f\"model.layers.{focus_layer}.mlp.up_proj.weight\"]\ndown_base = base_state_dict[f\"model.layers.{focus_layer}.mlp.down_proj.weight\"]\npre_ln_base = base_state_dict[f\"model.layers.{focus_layer}.pre_feedforward_layernorm.weight\"]\npost_ln_base = base_state_dict[f\"model.layers.{focus_layer}.post_feedforward_layernorm.weight\"]\n\ngate_inst = instruct_state_dict[f\"model.layers.{focus_layer}.mlp.gate_proj.weight\"]\nup_inst = instruct_state_dict[f\"model.layers.{focus_layer}.mlp.up_proj.weight\"]\ndown_inst = instruct_state_dict[f\"model.layers.{focus_layer}.mlp.down_proj.weight\"]\npre_ln_inst = instruct_state_dict[f\"model.layers.{focus_layer}.pre_feedforward_layernorm.weight\"]\npost_ln_inst = instruct_state_dict[f\"model.layers.{focus_layer}.post_feedforward_layernorm.weight\"]\n\nwith torch.inference_mode():\n    pc_batch = pc_vec.unsqueeze(0)\n    output_base = full_mlp_forward_batch(pc_batch, gate_base, up_base, down_base, pre_ln_base, post_ln_base)[0]\n    output_inst = full_mlp_forward_batch(pc_batch, gate_inst, up_inst, down_inst, pre_ln_inst, post_ln_inst)[0]\n    \n# Compute difference: what instruction tuning ADDS\ndifference = output_inst - output_base\n\nprint(f\"\\nLayer {focus_layer} {first_pc_name} MLP transformation:\")\nprint(f\"  Base output norm: {output_base.float().norm().item():.4f}\")\nprint(f\"  Instruct output norm: {output_inst.float().norm().item():.4f}\")\nprint(f\"  Difference norm: {difference.float().norm().item():.4f}\")\n\n# Project difference onto semantic vectors\nsemantic_names = list(all_semantic.keys())\nprojections = {}\nfor name in semantic_names:\n    vec = all_semantic[name]\n    proj = (difference.float() @ vec.float()).item()\n    projections[name] = proj\n\n# Sort by strength\nsorted_projs = sorted(projections.items(), key=lambda x: abs(x[1]), reverse=True)\n\nprint(f\"\\n\ud83d\udd0d Top {n_top_projections} semantic projections (what instruction tuning adds):\")\nprint(\"=\"*80)\nfor name, proj in sorted_projs[:n_top_projections]:\n    print(f\"{proj:+.4f}  {name}\")\n\nprint(f\"\\n\ud83d\udd0d Bottom {n_top_projections} semantic projections:\")\nprint(\"=\"*80)\nfor name, proj in sorted_projs[-n_top_projections:]:\n    print(f\"{proj:+.4f}  {name}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary",
   "metadata": {},
   "source": "## 4. Summary\n\nThis analysis reveals how instruction tuning modifies MLP transformations of PC vectors:\n\n**Key Insights:**\n- **Full MLP pass** reveals nonlinear transformation effects missed by linear analysis\n- **Layer-specific effects** show where instruction tuning has strongest impact\n- **Semantic decomposition** reveals which semantic directions are amplified/suppressed\n- **Focus layer** (originally layer 18) shows particularly strong semantic effects\n\nThe projections onto role/trait vectors reveal the semantic meaning of instruction tuning's modifications."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}