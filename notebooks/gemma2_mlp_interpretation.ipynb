{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma2-27B MLP Interpretation\n",
    "\n",
    "This notebook analyzes how PC vectors transform through the full MLP forward pass, including nonlinear activations.\n",
    "\n",
    "**Key Finding from Original Analysis:**\n",
    "- **Layer 18 is the \"smoking gun\"** - shows strongest PC1 transformation effects\n",
    "- Full MLP pass reveals semantic decomposition that linear analysis misses\n",
    "\n",
    "**Approach:**\n",
    "1. Run PC vectors through complete MLP (pre_ln \u2192 gate/up \u2192 down \u2192 post_ln)\n",
    "2. Compare base vs instruct outputs\n",
    "3. Decompose outputs onto role/trait semantic vectors\n",
    "4. Focus on layer 18 for detailed interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "from chatspace.analysis import (\n",
    "    load_pca_data,\n",
    "    extract_pc_components,\n",
    "    load_individual_role_vectors,\n",
    "    load_individual_trait_vectors,\n",
    "    normalize_vector,\n",
    "    full_mlp_forward_batch,\n",
    "    compute_z_scores\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models and PC Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models (reuse from basic_weight_susceptibility or reload)\n",
    "base_model_id = \"google/gemma-2-27b\"\n",
    "instruct_model_id = \"google/gemma-2-27b-it\"\n",
    "\n",
    "print(\"Loading models...\")\n",
    "config = AutoConfig.from_pretrained(base_model_id)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", low_cpu_mem_usage=True)\n",
    "instruct_model = AutoModelForCausalLM.from_pretrained(instruct_model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", low_cpu_mem_usage=True)\n",
    "base_state_dict = {k: v.cpu() for k, v in base_model.state_dict().items()}\n",
    "instruct_state_dict = {k: v.cpu() for k, v in instruct_model.state_dict().items()}\n",
    "print(\"\u2713 Models loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PCA data\n",
    "persona_data_root = Path(\"/workspace/persona-data\")\n",
    "roles_pca_dir = persona_data_root / \"gemma-2-27b\" / \"roles_240\" / \"pca\"\n",
    "traits_pca_dir = persona_data_root / \"gemma-2-27b\" / \"traits_240\" / \"pca\"\n",
    "\n",
    "pca_data, _ = load_pca_data(roles_pca_dir)\n",
    "pcs, variance_explained = extract_pc_components(pca_data, n_components=3)\n",
    "pc1 = pcs[0]\n",
    "print(f\"\u2713 Loaded PC1 from layer {pca_data['layer']}\")\n",
    "print(f\"  Variance explained: {variance_explained[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Full MLP Forward Pass Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MLP analysis across layers\n",
    "target_layers = range(15, 25)  # Focus on middle layers\n",
    "mlp_results = []\n",
    "\n",
    "print(f\"Running MLP forward pass for layers {list(target_layers)}...\")\n",
    "\n",
    "for layer_num in tqdm(target_layers):\n",
    "    # Get MLP weights\n",
    "    gate_base = base_state_dict[f\"model.layers.{layer_num}.mlp.gate_proj.weight\"]\n",
    "    up_base = base_state_dict[f\"model.layers.{layer_num}.mlp.up_proj.weight\"]\n",
    "    down_base = base_state_dict[f\"model.layers.{layer_num}.mlp.down_proj.weight\"]\n",
    "    pre_ln_base = base_state_dict[f\"model.layers.{layer_num}.pre_feedforward_layernorm.weight\"]\n",
    "    post_ln_base = base_state_dict[f\"model.layers.{layer_num}.post_feedforward_layernorm.weight\"]\n",
    "    \n",
    "    gate_inst = instruct_state_dict[f\"model.layers.{layer_num}.mlp.gate_proj.weight\"]\n",
    "    up_inst = instruct_state_dict[f\"model.layers.{layer_num}.mlp.up_proj.weight\"]\n",
    "    down_inst = instruct_state_dict[f\"model.layers.{layer_num}.mlp.down_proj.weight\"]\n",
    "    pre_ln_inst = instruct_state_dict[f\"model.layers.{layer_num}.pre_feedforward_layernorm.weight\"]\n",
    "    post_ln_inst = instruct_state_dict[f\"model.layers.{layer_num}.post_feedforward_layernorm.weight\"]\n",
    "    \n",
    "    # Prepare input vectors\n",
    "    vectors = torch.stack([pc1, -pc1])  # PC1 and -PC1\n",
    "    vector_names = ['PC1', '-PC1']\n",
    "    \n",
    "    # Forward pass through base and instruct\n",
    "    with torch.inference_mode():\n",
    "        output_base = full_mlp_forward_batch(vectors, gate_base, up_base, down_base, pre_ln_base, post_ln_base)\n",
    "        output_inst = full_mlp_forward_batch(vectors, gate_inst, up_inst, down_inst, pre_ln_inst, post_ln_inst)\n",
    "    \n",
    "    # Compute norms and delta\n",
    "    for i, vec_name in enumerate(vector_names):\n",
    "        base_norm = output_base[i].float().norm().item()\n",
    "        inst_norm = output_inst[i].float().norm().item()\n",
    "        delta = (output_inst[i] - output_base[i]).float().norm().item()\n",
    "        \n",
    "        mlp_results.append({\n",
    "            'layer': layer_num,\n",
    "            'input_vector': vec_name,\n",
    "            'base_output_norm': base_norm,\n",
    "            'instruct_output_norm': inst_norm,\n",
    "            'delta_norm': delta\n",
    "        })\n",
    "\n",
    "mlp_df = pd.DataFrame(mlp_results)\n",
    "print(f\"\\n\u2713 MLP analysis complete: {len(mlp_df)} results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Delta norm by layer\n",
    "for vec_name in ['PC1', '-PC1']:\n",
    "    data = mlp_df[mlp_df['input_vector'] == vec_name]\n",
    "    axes[0].plot(data['layer'], data['delta_norm'], marker='o', label=vec_name)\n",
    "axes[0].set_title('MLP Output Delta (Instruct - Base) by Layer')\n",
    "axes[0].set_xlabel('Layer')\n",
    "axes[0].set_ylabel('Delta Norm')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Compare base vs instruct norms\n",
    "pc1_data = mlp_df[mlp_df['input_vector'] == 'PC1']\n",
    "axes[1].plot(pc1_data['layer'], pc1_data['base_output_norm'], marker='o', label='Base')\n",
    "axes[1].plot(pc1_data['layer'], pc1_data['instruct_output_norm'], marker='s', label='Instruct')\n",
    "axes[1].set_title('MLP Output Norms for PC1')\n",
    "axes[1].set_xlabel('Layer')\n",
    "axes[1].set_ylabel('Output Norm')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find peak layer\n",
    "peak_layer = mlp_df[mlp_df['input_vector'] == 'PC1']['delta_norm'].idxmax()\n",
    "peak_layer_num = mlp_df.loc[peak_layer, 'layer']\n",
    "print(f\"\\n\ud83c\udfaf Peak delta at layer {peak_layer_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Layer 18 Semantic Decomposition\n\nOriginal analysis identified layer 18 as having the strongest effect. Let's decompose what instruction tuning adds to PC1's transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on layer 18\n",
    "target_layer = 18\n",
    "\n",
    "# Load individual role and trait vectors for this layer\n",
    "persona_data_root = Path(\"/workspace/persona-data\")\n",
    "roles_vectors_dir = persona_data_root / \"gemma-2-27b\" / \"roles_240\" / \"vectors\"\n",
    "traits_vectors_dir = persona_data_root / \"gemma-2-27b\" / \"traits_240\" / \"vectors\"\n",
    "\n",
    "role_vectors = load_individual_role_vectors(roles_vectors_dir, target_layer, vector_type='pos_all')\n",
    "trait_vectors = load_individual_trait_vectors(traits_vectors_dir, target_layer, vector_type='pos_all')\n",
    "\n",
    "print(f\"Loaded semantic vectors for layer {target_layer}:\")\n",
    "print(f\"  Roles: {len(role_vectors)}\")\n",
    "print(f\"  Traits: {len(trait_vectors)}\")\n",
    "\n",
    "# Combine all semantic vectors\n",
    "all_semantic = {**role_vectors, **trait_vectors}\n",
    "semantic_names = list(all_semantic.keys())\n",
    "semantic_matrix = torch.stack([all_semantic[name] for name in semantic_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PC1 through layer 18 MLP\n",
    "gate_base = base_state_dict[f\"model.layers.{target_layer}.mlp.gate_proj.weight\"]\n",
    "up_base = base_state_dict[f\"model.layers.{target_layer}.mlp.up_proj.weight\"]\n",
    "down_base = base_state_dict[f\"model.layers.{target_layer}.mlp.down_proj.weight\"]\n",
    "pre_ln_base = base_state_dict[f\"model.layers.{target_layer}.pre_feedforward_layernorm.weight\"]\n",
    "post_ln_base = base_state_dict[f\"model.layers.{target_layer}.post_feedforward_layernorm.weight\"]\n",
    "\n",
    "gate_inst = instruct_state_dict[f\"model.layers.{target_layer}.mlp.gate_proj.weight\"]\n",
    "up_inst = instruct_state_dict[f\"model.layers.{target_layer}.mlp.up_proj.weight\"]\n",
    "down_inst = instruct_state_dict[f\"model.layers.{target_layer}.mlp.down_proj.weight\"]\n",
    "pre_ln_inst = instruct_state_dict[f\"model.layers.{target_layer}.pre_feedforward_layernorm.weight\"]\n",
    "post_ln_inst = instruct_state_dict[f\"model.layers.{target_layer}.post_feedforward_layernorm.weight\"]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    pc1_batch = pc1.unsqueeze(0)\n",
    "    output_base = full_mlp_forward_batch(pc1_batch, gate_base, up_base, down_base, pre_ln_base, post_ln_base)[0]\n",
    "    output_inst = full_mlp_forward_batch(pc1_batch, gate_inst, up_inst, down_inst, pre_ln_inst, post_ln_inst)[0]\n",
    "    \n",
    "# Compute difference: what instruction tuning ADDS\n",
    "difference = output_inst - output_base\n",
    "\n",
    "print(f\"\\nLayer {target_layer} PC1 MLP transformation:\")\n",
    "print(f\"  Base output norm: {output_base.float().norm().item():.4f}\")\n",
    "print(f\"  Instruct output norm: {output_inst.float().norm().item():.4f}\")\n",
    "print(f\"  Difference norm: {difference.float().norm().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project difference onto semantic vectors\n",
    "projections = {}\n",
    "for name in semantic_names:\n",
    "    vec = all_semantic[name]\n",
    "    proj = (difference.float() @ vec.float()).item()\n",
    "    projections[name] = proj\n",
    "\n",
    "# Sort by strength\n",
    "sorted_projs = sorted(projections.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print(f\"\\n\ud83d\udd0d Top 15 semantic projections (what instruction tuning adds):\")\n",
    "print(\"=\"*80)\n",
    "for name, proj in sorted_projs[:15]:\n",
    "    print(f\"{proj:+.4f}  {name}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udd0d Bottom 15 semantic projections:\")\n",
    "print(\"=\"*80)\n",
    "for name, proj in sorted_projs[-15:]:\n",
    "    print(f\"{proj:+.4f}  {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary\n\nThis analysis reveals how instruction tuning modifies the MLP transformation of PC vectors, with layer 18 showing particularly strong semantic effects. The projections onto role/trait vectors reveal which semantic directions are amplified or suppressed by instruction tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}