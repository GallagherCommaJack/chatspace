{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9706870",
   "metadata": {},
   "source": [
    "# Steering Vector Training & Evaluation Workflow\n",
    "\n",
    "This notebook mirrors the logic in `chatspace/steering/train.py` and `scripts/generate_behavior_rollouts.py` to train, load, and evaluate steering vectors for persona datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeba2260",
   "metadata": {},
   "source": [
    "## Notebook Outline\n",
    "- Environment configuration\n",
    "- Base model loading helpers\n",
    "- Training utilities\n",
    "- Vector loaders (trained + activation)\n",
    "- Rollout generation and persistence\n",
    "- Example usage for training, loading, and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54bd0c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from argparse import Namespace\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Sequence\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl.trainer.sft_trainer import SFTConfig, SFTTrainer\n",
    "\n",
    "from chatspace.steering.data import PersonaSteeringDatasetConfig, load_persona_steering_dataset\n",
    "from chatspace.steering.model import QwenSteerModel, SteeringVectorConfig\n",
    "from chatspace.steering.train import EarlyStopCallback, _compute_average_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5099fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default paths follow the CLI expectations in chatspace. Adjust as needed.\n",
    "MODEL_NAME = \"Qwen/Qwen3-32B\"\n",
    "TARGET_LAYER = 30\n",
    "PERSONA_ROOT = Path(\"/workspace/persona-data\")\n",
    "RUN_ROOT = Path(f\"/workspace/steering_runs_qwen3_layer_{TARGET_LAYER}\")\n",
    "ROLLOUTS_PATH = Path(f\"/workspace/steering_rollouts_qwen3_layer_{TARGET_LAYER}/notebook_rollouts.jsonl\")\n",
    "DEFAULT_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ROLLOUTS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Replace with an actual dataset when ready, e.g. \"qwen-3-32b__trait__agreeable\"\n",
    "DATASET_NAME: str | None = \"qwen-3-32b__trait__acerbic\"\n",
    "\n",
    "_BASE_MODEL_CACHE: dict[tuple[str, str], tuple[AutoModelForCausalLM, AutoTokenizer]] = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a9fd8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_SHARED_TRAIN_COMPONENTS: dict[tuple[str, int, str], tuple[QwenSteerModel, AutoTokenizer]] = {}\n",
    "\n",
    "\n",
    "def _device_cache_key(device_map: str | dict | None) -> str:\n",
    "    if isinstance(device_map, dict):\n",
    "        # Sort keys for deterministic cache key\n",
    "        return json.dumps(device_map, sort_keys=True)\n",
    "    return str(device_map)\n",
    "\n",
    "\n",
    "def get_training_components(\n",
    "    model_name: str = MODEL_NAME,\n",
    "    target_layer: int = TARGET_LAYER,\n",
    "    *,\n",
    "    device_map: str | dict | None = \"auto\",\n",
    ") -> tuple[QwenSteerModel, AutoTokenizer]:\n",
    "    \"\"\"Load (or reuse) the shared Qwen steering model and tokenizer for training.\"\"\"\n",
    "\n",
    "    cache_key = (model_name, target_layer, _device_cache_key(device_map))\n",
    "    cached = _SHARED_TRAIN_COMPONENTS.get(cache_key)\n",
    "    if cached is not None:\n",
    "        return cached\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    model_kwargs: dict[str, object] = {\"torch_dtype\": \"auto\"}\n",
    "    if device_map == \"cuda\":\n",
    "        model_kwargs[\"device_map\"] = None\n",
    "    else:\n",
    "        model_kwargs[\"device_map\"] = device_map\n",
    "        if device_map == \"auto\":\n",
    "            model_kwargs[\"low_cpu_mem_usage\"] = False\n",
    "\n",
    "    cfg = SteeringVectorConfig(\n",
    "        model_name=model_name,\n",
    "        target_layer=target_layer,\n",
    "        init_scale=0.0,\n",
    "    )\n",
    "    model = QwenSteerModel(cfg, **model_kwargs)\n",
    "    if device_map == \"cuda\" and torch.cuda.is_available():\n",
    "        model = model.to(torch.device(\"cuda\"))\n",
    "\n",
    "    _SHARED_TRAIN_COMPONENTS[cache_key] = (model, tokenizer)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def reset_shared_vector(model: QwenSteerModel, init_scale: float) -> None:\n",
    "    \"\"\"Reset the shared steering vector inplace prior to a new training run.\"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if init_scale == 0.0:\n",
    "            model.steering.vector.zero_()\n",
    "        else:\n",
    "            torch.nn.init.normal_(model.steering.vector, mean=0.0, std=init_scale)\n",
    "    if model.steering.vector.grad is not None:\n",
    "        model.steering.vector.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DEFAULTS = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"target_layer\": TARGET_LAYER,\n",
    "    \"seed\": 17,\n",
    "    \"learning_rate\": 5e-1,\n",
    "    \"init_scale\": 0.0,\n",
    "    \"batch_size\": 4,\n",
    "    \"gradient_accumulation\": 1,\n",
    "    \"max_length\": 4096,\n",
    "    \"target_tokens\": 100_000,\n",
    "    \"val_target_tokens\": 10_000,\n",
    "    \"role_score\": 3,\n",
    "    \"trait_score\": 75,\n",
    "    \"warmup_ratio\": 0.05,\n",
    "    \"bf16\": torch.cuda.is_available(),\n",
    "    \"max_steps\": -1,\n",
    "    \"num_epochs\": 5.0,\n",
    "    \"gradient_checkpointing\": False,\n",
    "    \"device_map\": \"auto\",\n",
    "    \"logging_steps\": 10,\n",
    "    \"lr_scheduler\": \"constant\",\n",
    "    \"early_stop_patience\": 3,\n",
    "    \"early_stop_threshold\": 0.0,\n",
    "    \"compare_prompted\": False,\n",
    "}\n",
    "\n",
    "\n",
    "def build_training_args(dataset_name: str, output_dir: Path, **overrides) -> Namespace:\n",
    "    \"\"\"Create an argparse-style namespace compatible with the trainer helpers.\"\"\"\n",
    "\n",
    "    params = {**TRAINING_DEFAULTS, **overrides}\n",
    "    params[\"datasets\"] = [dataset_name]\n",
    "    params[\"output_dir\"] = Path(output_dir)\n",
    "    return Namespace(**params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c874548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base_model(\n",
    "    model_name: str = MODEL_NAME,\n",
    "    *,\n",
    "    device_map: str | dict | None = \"auto\",\n",
    ") -> tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"Load (or reuse) the base causal LM and tokenizer used for steering experiments.\"\"\"\n",
    "\n",
    "    cache_key = (model_name, _device_cache_key(device_map))\n",
    "    cached = _BASE_MODEL_CACHE.get(cache_key)\n",
    "    if cached is not None:\n",
    "        return cached\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=device_map,\n",
    "        low_cpu_mem_usage=False,\n",
    "    )\n",
    "    if device_map in (None, \"cpu\"):\n",
    "        model = model.to(DEFAULT_DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    cache_value = (model, tokenizer)\n",
    "    _BASE_MODEL_CACHE[cache_key] = cache_value\n",
    "    return cache_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92d03e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_steering_vector(\n",
    "    dataset_name: str,\n",
    "    *,\n",
    "    output_root: Path = RUN_ROOT,\n",
    "    run_validation: bool = True,\n",
    "    **overrides,\n",
    ") -> dict[str, object]:\n",
    "    \"\"\"Train a steering vector for the given dataset and return training metadata.\"\"\"\n",
    "\n",
    "    output_dir = Path(output_root) / dataset_name\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    args = build_training_args(dataset_name, output_dir, **overrides)\n",
    "\n",
    "    model, tokenizer = get_training_components(\n",
    "        model_name=args.model,\n",
    "        target_layer=args.target_layer,\n",
    "        device_map=args.device_map,\n",
    "    )\n",
    "\n",
    "    reset_shared_vector(model, args.init_scale)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    dataset_cfg = PersonaSteeringDatasetConfig(\n",
    "        dataset_names=[dataset_name],\n",
    "        target_tokens=args.target_tokens + max(args.val_target_tokens, 0),\n",
    "        seed=args.seed,\n",
    "        tokenizer_name=args.model,\n",
    "        max_length=args.max_length,\n",
    "        role_min_score=args.role_score,\n",
    "        trait_min_score=args.trait_score,\n",
    "    )\n",
    "    full_dataset = load_persona_steering_dataset(dataset_cfg, tokenizer)\n",
    "\n",
    "    token_lengths = list(full_dataset[\"length\"])\n",
    "    target_tokens = args.target_tokens\n",
    "    val_tokens = max(args.val_target_tokens, 0)\n",
    "    cumulative = 0\n",
    "    train_indices: list[int] = []\n",
    "    val_indices: list[int] = []\n",
    "\n",
    "    for idx, length in enumerate(token_lengths):\n",
    "        cumulative += int(length)\n",
    "        if cumulative <= target_tokens:\n",
    "            train_indices.append(idx)\n",
    "        elif val_tokens > 0 and cumulative <= target_tokens + val_tokens:\n",
    "            val_indices.append(idx)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if not train_indices:\n",
    "        raise ValueError(\n",
    "            \"Unable to allocate any training examples; increase target tokens or relax filters\",\n",
    "        )\n",
    "\n",
    "    train_dataset = full_dataset.select(train_indices)\n",
    "    train_tokens = sum(int(full_dataset[i][\"length\"]) for i in train_indices)\n",
    "\n",
    "    val_dataset = None\n",
    "    val_selected_tokens = 0\n",
    "    if val_indices:\n",
    "        val_dataset = full_dataset.select(val_indices)\n",
    "        val_selected_tokens = sum(int(full_dataset[i][\"length\"]) for i in val_indices)\n",
    "\n",
    "    message = (\n",
    "        f\"Prepared dataset with {len(train_dataset)} train sequences / {train_tokens} tokens\"\n",
    "    )\n",
    "    if val_dataset is not None:\n",
    "        message += f\"; validation {len(val_dataset)} sequences / {val_selected_tokens} tokens\"\n",
    "    print(message + \".\")\n",
    "\n",
    "    eval_strategy = \"epoch\" if val_dataset is not None else \"no\"\n",
    "\n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=str(output_dir),\n",
    "        seed=args.seed,\n",
    "        do_eval=val_dataset is not None,\n",
    "        learning_rate=args.learning_rate,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation,\n",
    "        max_steps=args.max_steps,\n",
    "        bf16=args.bf16,\n",
    "        num_train_epochs=args.num_epochs,\n",
    "        logging_steps=max(1, args.logging_steps),\n",
    "        eval_strategy=eval_strategy,\n",
    "        save_strategy=\"no\",\n",
    "        warmup_ratio=args.warmup_ratio,\n",
    "        report_to=[],\n",
    "        gradient_checkpointing=args.gradient_checkpointing,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False} if args.gradient_checkpointing else None,\n",
    "        lr_scheduler_type=args.lr_scheduler,\n",
    "        save_only_model=True,\n",
    "        save_total_limit=1,\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=sft_config,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.create_model_card = lambda *_, **__: None\n",
    "    output_dir_path = output_dir\n",
    "\n",
    "    def _save_model(save_path: str | None = None, _internal_call: bool = False) -> None:\n",
    "        target_dir = Path(save_path) if save_path is not None else output_dir_path\n",
    "        model.save_pretrained(target_dir)\n",
    "\n",
    "    trainer.save_model = _save_model  # type: ignore[assignment]\n",
    "    trainer._val_dataset = val_dataset  # type: ignore[attr-defined]\n",
    "    trainer._tokenizer = tokenizer  # type: ignore[attr-defined]\n",
    "\n",
    "    stop_callback = None\n",
    "    if val_dataset is not None and args.early_stop_patience > 0:\n",
    "        stop_callback = EarlyStopCallback(trainer, args.early_stop_patience, args.early_stop_threshold)\n",
    "        trainer.add_callback(stop_callback)\n",
    "        trainer._early_stop_callback = stop_callback  # type: ignore[attr-defined]\n",
    "    else:\n",
    "        trainer._early_stop_callback = None  # type: ignore[attr-defined]\n",
    "\n",
    "    train_output = trainer.train()\n",
    "\n",
    "    if stop_callback is not None and getattr(stop_callback, \"best_vector\", None) is not None:\n",
    "        vector = stop_callback.best_vector.to(model.steering.vector.device)\n",
    "        model.steering.vector.data.copy_(vector)\n",
    "\n",
    "    eval_metrics = None\n",
    "    if run_validation and val_dataset is not None:\n",
    "        eval_metrics = trainer.evaluate()\n",
    "        if \"eval_loss\" not in eval_metrics:\n",
    "            eval_loss = _compute_average_loss(model, trainer.get_eval_dataloader())\n",
    "            eval_metrics[\"eval_loss\"] = eval_loss\n",
    "        if \"eval_loss\" in eval_metrics and \"eval_ppl\" not in eval_metrics:\n",
    "            eval_metrics[\"eval_ppl\"] = math.exp(eval_metrics[\"eval_loss\"])\n",
    "\n",
    "    trainer.save_state()\n",
    "    trainer.save_model()\n",
    "\n",
    "    return {\n",
    "        \"output_dir\": output_dir,\n",
    "        \"train_metrics\": getattr(train_output, \"metrics\", None),\n",
    "        \"eval_metrics\": eval_metrics,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d599f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_vector(\n",
    "    dataset_name: str,\n",
    "    *,\n",
    "    run_root: Path = RUN_ROOT,\n",
    "    map_location: str | torch.device = \"cpu\",\n",
    ") -> dict[str, object]:\n",
    "    \"\"\"Load a previously trained steering vector and metadata for the dataset.\"\"\"\n",
    "\n",
    "    steering_dir = Path(run_root) / dataset_name\n",
    "    vector_path = steering_dir / \"steering_vector.pt\"\n",
    "    config_path = steering_dir / \"steering_config.json\"\n",
    "\n",
    "    result = {\n",
    "        \"vector\": None,\n",
    "        \"layer\": TARGET_LAYER,\n",
    "        \"path\": vector_path,\n",
    "        \"norm\": None,\n",
    "    }\n",
    "\n",
    "    if not vector_path.exists():\n",
    "        return result\n",
    "\n",
    "    state = torch.load(vector_path, map_location=map_location)\n",
    "    tensor = state.get(\"steering_vector\")\n",
    "    if tensor is None:\n",
    "        raise ValueError(f\"steering_vector.pt missing 'steering_vector' key at {vector_path}\")\n",
    "    vector = tensor.float()\n",
    "    result[\"vector\"] = vector\n",
    "    result[\"norm\"] = float(torch.linalg.norm(vector).item()) if vector.numel() > 0 else None\n",
    "\n",
    "    if config_path.exists():\n",
    "        cfg = json.loads(config_path.read_text())\n",
    "        result[\"layer\"] = int(cfg.get(\"target_layer\", TARGET_LAYER))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_activation_vector(\n",
    "    dataset_name: str,\n",
    "    *,\n",
    "    persona_root: Path = PERSONA_ROOT,\n",
    "    target_layer: int = TARGET_LAYER,\n",
    "    map_location: str | torch.device = \"cpu\",\n",
    ") -> dict[str, object]:\n",
    "    \"\"\"Load the canonical (vanilla) activation vector for the dataset at the target layer.\"\"\"\n",
    "\n",
    "    result = {\n",
    "        \"vector\": None,\n",
    "        \"layer\": target_layer,\n",
    "        \"path\": None,\n",
    "        \"norm\": None,\n",
    "    }\n",
    "\n",
    "    if \"__trait__\" in dataset_name:\n",
    "        model_prefix, trait = dataset_name.split(\"__trait__\", 1)\n",
    "        vec_file = persona_root / f\"{model_prefix}/traits_240/vectors/{trait}.pt\"\n",
    "        result[\"path\"] = vec_file\n",
    "        if not vec_file.exists():\n",
    "            return result\n",
    "        data = torch.load(vec_file, map_location=map_location)\n",
    "        vec = data[\"pos_neg_50\"][target_layer]\n",
    "    elif \"__role__\" in dataset_name:\n",
    "        role = dataset_name.split(\"__role__\", 1)[1]\n",
    "        vec_file = persona_root / f\"qwen-3-32b/roles_240/vectors/{role}.pt\"\n",
    "        result[\"path\"] = vec_file\n",
    "        if not vec_file.exists():\n",
    "            return result\n",
    "        data = torch.load(vec_file, map_location=map_location)\n",
    "        vec_pos = data[\"pos_3\"][target_layer]\n",
    "        vec_default = data[\"default_1\"][target_layer]\n",
    "        vec = vec_pos - vec_default\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognized dataset name: {dataset_name}\")\n",
    "\n",
    "    vector = vec.float()\n",
    "    result[\"vector\"] = vector\n",
    "    result[\"norm\"] = float(torch.linalg.norm(vector).item()) if vector.numel() > 0 else None\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc9b50b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteeringController:\n",
    "    \"\"\"Attach a single residual hook and swap steering vectors on demand.\"\"\"\n",
    "\n",
    "    def __init__(self, model: AutoModelForCausalLM) -> None:\n",
    "        self.model = model\n",
    "        self.layer_idx: int | None = None\n",
    "        self._handle = None\n",
    "        self.vector: torch.Tensor | None = None\n",
    "\n",
    "    def _hook(self, module, args, output):\n",
    "        if self.vector is None:\n",
    "            return output\n",
    "        hidden = output[0] if isinstance(output, tuple) else output\n",
    "        vec = self.vector\n",
    "        if vec.device != hidden.device or vec.dtype != hidden.dtype:\n",
    "            vec = vec.to(device=hidden.device, dtype=hidden.dtype)\n",
    "            self.vector = vec\n",
    "        steered = hidden + vec\n",
    "        if isinstance(output, tuple):\n",
    "            return (steered,) + output[1:]\n",
    "        return steered\n",
    "\n",
    "    def set_layer(self, layer_idx: int) -> None:\n",
    "        if self.layer_idx == layer_idx:\n",
    "            return\n",
    "        if self._handle is not None:\n",
    "            self._handle.remove()\n",
    "        layer = self.model.model.layers[layer_idx]\n",
    "        self._handle = layer.register_forward_hook(self._hook)\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "    def set_vector(self, vector: torch.Tensor | None) -> None:\n",
    "        if vector is None:\n",
    "            self.vector = None\n",
    "            return\n",
    "        if vector.ndim != 1:\n",
    "            raise ValueError(\"Steering vector must be 1D\")\n",
    "        self.vector = vector\n",
    "\n",
    "    def close(self) -> None:\n",
    "        if self._handle is not None:\n",
    "            self._handle.remove()\n",
    "            self._handle = None\n",
    "            self.layer_idx = None\n",
    "\n",
    "\n",
    "def _prepare_scaled_variants(\n",
    "    base_name: str,\n",
    "    vector: torch.Tensor | None,\n",
    "    scales: Sequence[float],\n",
    "    normalize: bool,\n",
    "    *,\n",
    "    include_learned: bool = False,\n",
    ") -> list[tuple[str, torch.Tensor, float]]:\n",
    "    if vector is None:\n",
    "        return []\n",
    "\n",
    "    vec = vector\n",
    "    norm = float(torch.linalg.norm(vec).item())\n",
    "    if normalize:\n",
    "        if norm > 0:\n",
    "            vec = vec / norm\n",
    "        else:\n",
    "            normalize = False\n",
    "\n",
    "    variants: list[tuple[str, torch.Tensor, float]] = []\n",
    "    if include_learned:\n",
    "        learned_name = f\"{base_name}_scale_learned\"\n",
    "        if normalize and norm > 0:\n",
    "            variants.append((learned_name, vec * norm, norm))\n",
    "        else:\n",
    "            variants.append((learned_name, vector, 1.0))\n",
    "    existing_names = {name for name, _, _ in variants}\n",
    "    for scale in scales:\n",
    "        scaled = vec * float(scale)\n",
    "        scaled_name = base_name\n",
    "        if not (len(scales) == 1 and abs(scale - 1.0) < 1e-6):\n",
    "            scaled_name = f\"{base_name}_scale_{scale:g}\"\n",
    "        if scaled_name in existing_names:\n",
    "            continue\n",
    "        variants.append((scaled_name, scaled, float(scale)))\n",
    "        existing_names.add(scaled_name)\n",
    "    return variants\n",
    "\n",
    "\n",
    "def make_messages(\n",
    "    system_prompt: str | None,\n",
    "    question: str,\n",
    "    include_system: bool = True,\n",
    ") -> list[dict[str, str]]:\n",
    "    msgs: list[dict[str, str]] = []\n",
    "    if include_system and system_prompt:\n",
    "        msgs.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    msgs.append({\"role\": \"user\", \"content\": question})\n",
    "    return msgs\n",
    "\n",
    "\n",
    "def append_rollouts(records: Iterable[dict], rollouts_path: Path = ROLLOUTS_PATH) -> None:\n",
    "    rollouts_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with rollouts_path.open(\"a\", encoding=\"utf-8\") as fh:\n",
    "        for rec in records:\n",
    "            fh.write(json.dumps(rec) + \"\n",
    "\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RolloutGenerationConfig:\n",
    "    max_new_tokens: int = 256\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.95\n",
    "    do_sample: bool = True\n",
    "\n",
    "\n",
    "def generate_dataset_rollouts(\n",
    "    dataset_name: str,\n",
    "    *,\n",
    "    num_rollouts: int = 1,\n",
    "    trained_scales: Sequence[float] = (1.0,),\n",
    "    activation_scales: Sequence[float] = (1.0,),\n",
    "    normalize_steering: bool = True,\n",
    "    include_prompted: bool = True,\n",
    "    gen_config: RolloutGenerationConfig | None = None,\n",
    "    rollouts_path: Path = ROLLOUTS_PATH,\n",
    "    model_name: str = MODEL_NAME,\n",
    "    device_map: str | dict | None = \"auto\",\n",
    ") -> list[dict[str, object]]:\n",
    "    \"\"\"Run baseline/trained/activation rollouts using a cached base model and append them to rollouts_path.\"\"\"\n",
    "\n",
    "    from scripts import generate_behavior_rollouts as rollout_script\n",
    "\n",
    "    model, tokenizer = load_base_model(model_name=model_name, device_map=device_map)\n",
    "\n",
    "    instructions = rollout_script.load_instructions(dataset_name)\n",
    "    timestamp = datetime.now(timezone.utc).isoformat()\n",
    "    cfg = gen_config or RolloutGenerationConfig()\n",
    "\n",
    "    controller = SteeringController(model)\n",
    "    records: list[dict[str, object]] = []\n",
    "\n",
    "    try:\n",
    "        trained_info = load_trained_vector(dataset_name)\n",
    "        activation_info = load_activation_vector(dataset_name)\n",
    "\n",
    "        trained_vector = trained_info[\"vector\"]\n",
    "        trained_layer = trained_info[\"layer\"] if trained_info[\"layer\"] is not None else TARGET_LAYER\n",
    "        trained_norm = trained_info[\"norm\"]\n",
    "\n",
    "        activation_vector = activation_info[\"vector\"]\n",
    "        activation_layer = activation_info[\"layer\"] if activation_info[\"layer\"] is not None else TARGET_LAYER\n",
    "\n",
    "        trained_variants = _prepare_scaled_variants(\n",
    "            \"trained\",\n",
    "            trained_vector,\n",
    "            trained_scales,\n",
    "            normalize_steering,\n",
    "            include_learned=True,\n",
    "        )\n",
    "        activation_variants = _prepare_scaled_variants(\n",
    "            \"activation\",\n",
    "            activation_vector,\n",
    "            activation_scales,\n",
    "            normalize_steering,\n",
    "            include_learned=False,\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            normalize_steering\n",
    "            and activation_vector is not None\n",
    "            and trained_norm is not None\n",
    "            and trained_norm > 0\n",
    "        ):\n",
    "            act_norm = float(torch.linalg.norm(activation_vector).item())\n",
    "            base_vec = activation_vector / act_norm if act_norm > 0 else activation_vector\n",
    "            learned_name = \"activation_scale_learned\"\n",
    "            neg_name = \"activation_scale_-learned\"\n",
    "            existing = {name for name, _, _ in activation_variants}\n",
    "            if learned_name not in existing:\n",
    "                activation_variants.append((learned_name, base_vec * trained_norm, trained_norm))\n",
    "            if neg_name not in existing:\n",
    "                activation_variants.append((neg_name, base_vec * (-trained_norm), -trained_norm))\n",
    "\n",
    "        baseline_layer = TARGET_LAYER\n",
    "        if trained_variants:\n",
    "            baseline_layer = trained_layer\n",
    "        elif activation_variants:\n",
    "            baseline_layer = activation_layer\n",
    "\n",
    "        parameter = next(model.parameters())\n",
    "        device = parameter.device\n",
    "\n",
    "        def run_batch(message_batches: list[list[dict[str, str]]], vector: torch.Tensor | None, layer_idx: int) -> list[str]:\n",
    "            controller.set_layer(layer_idx)\n",
    "            controller.set_vector(vector)\n",
    "            chat_texts = [\n",
    "                tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "                for msgs in message_batches\n",
    "            ]\n",
    "            encoded = tokenizer(chat_texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "            attention_mask = encoded.get(\"attention_mask\")\n",
    "            if attention_mask is None:\n",
    "                input_lens = torch.tensor([enc.size(0) for enc in encoded[\"input_ids\"]], device=device)\n",
    "            else:\n",
    "                input_lens = attention_mask.sum(dim=1)\n",
    "            outputs = model.generate(\n",
    "                **encoded,\n",
    "                max_new_tokens=cfg.max_new_tokens,\n",
    "                do_sample=cfg.do_sample,\n",
    "                temperature=cfg.temperature,\n",
    "                top_p=cfg.top_p,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "            texts: list[str] = []\n",
    "            for idx in range(outputs.size(0)):\n",
    "                seq = outputs[idx]\n",
    "                offset = int(input_lens[idx])\n",
    "                decoded = tokenizer.decode(seq[offset:], skip_special_tokens=True).strip()\n",
    "                texts.append(decoded)\n",
    "            return texts\n",
    "\n",
    "        if include_prompted:\n",
    "            for prompt_idx, prompt in enumerate(instructions.prompts):\n",
    "                batches = [make_messages(prompt, question) for question in instructions.questions]\n",
    "                for rollout_idx in range(num_rollouts):\n",
    "                    responses = run_batch(batches, vector=None, layer_idx=baseline_layer)\n",
    "                    for question_idx, (question, response) in enumerate(zip(instructions.questions, responses)):\n",
    "                        records.append({\n",
    "                            \"timestamp_utc\": timestamp,\n",
    "                            \"dataset\": dataset_name,\n",
    "                            \"variant\": \"prompted\",\n",
    "                            \"steering_source\": \"baseline\",\n",
    "                            \"steering_scale\": 0.0,\n",
    "                            \"steering_layer\": baseline_layer,\n",
    "                            \"steering_norm\": None,\n",
    "                            \"prompt_index\": prompt_idx,\n",
    "                            \"question_index\": question_idx,\n",
    "                            \"rollout_index\": rollout_idx,\n",
    "                            \"question\": question,\n",
    "                            \"system_prompt\": prompt,\n",
    "                            \"response\": response,\n",
    "                            \"model_name\": model_name,\n",
    "                            \"max_new_tokens\": cfg.max_new_tokens,\n",
    "                            \"temperature\": cfg.temperature,\n",
    "                            \"top_p\": cfg.top_p,\n",
    "                        })\n",
    "\n",
    "        shared_system = instructions.prompts[0] if instructions.prompts else None\n",
    "        shared_batches = [make_messages(shared_system, q) for q in instructions.questions]\n",
    "\n",
    "        for variant_name, variant_vec, scale in trained_variants:\n",
    "            for rollout_idx in range(num_rollouts):\n",
    "                responses = run_batch(shared_batches, vector=variant_vec, layer_idx=trained_layer)\n",
    "                norm_val = float(torch.linalg.norm(variant_vec).item()) if variant_vec is not None else None\n",
    "                for question_idx, (question, response) in enumerate(zip(instructions.questions, responses)):\n",
    "                    records.append({\n",
    "                        \"timestamp_utc\": timestamp,\n",
    "                        \"dataset\": dataset_name,\n",
    "                        \"variant\": variant_name,\n",
    "                        \"steering_source\": \"trained\",\n",
    "                        \"steering_scale\": scale,\n",
    "                        \"steering_layer\": trained_layer,\n",
    "                        \"steering_norm\": norm_val,\n",
    "                        \"prompt_index\": None,\n",
    "                        \"question_index\": question_idx,\n",
    "                        \"rollout_index\": rollout_idx,\n",
    "                        \"question\": question,\n",
    "                        \"system_prompt\": shared_system,\n",
    "                        \"response\": response,\n",
    "                        \"model_name\": model_name,\n",
    "                        \"max_new_tokens\": cfg.max_new_tokens,\n",
    "                        \"temperature\": cfg.temperature,\n",
    "                        \"top_p\": cfg.top_p,\n",
    "                    })\n",
    "\n",
    "        for variant_name, variant_vec, scale in activation_variants:\n",
    "            for rollout_idx in range(num_rollouts):\n",
    "                responses = run_batch(shared_batches, vector=variant_vec, layer_idx=activation_layer)\n",
    "                norm_val = float(torch.linalg.norm(variant_vec).item()) if variant_vec is not None else None\n",
    "                for question_idx, (question, response) in enumerate(zip(instructions.questions, responses)):\n",
    "                    records.append({\n",
    "                        \"timestamp_utc\": timestamp,\n",
    "                        \"dataset\": dataset_name,\n",
    "                        \"variant\": variant_name,\n",
    "                        \"steering_source\": \"activation\",\n",
    "                        \"steering_scale\": scale,\n",
    "                        \"steering_layer\": activation_layer,\n",
    "                        \"steering_norm\": norm_val,\n",
    "                        \"prompt_index\": None,\n",
    "                        \"question_index\": question_idx,\n",
    "                        \"rollout_index\": rollout_idx,\n",
    "                        \"question\": question,\n",
    "                        \"system_prompt\": shared_system,\n",
    "                        \"response\": response,\n",
    "                        \"model_name\": model_name,\n",
    "                        \"max_new_tokens\": cfg.max_new_tokens,\n",
    "                        \"temperature\": cfg.temperature,\n",
    "                        \"top_p\": cfg.top_p,\n",
    "                    })\n",
    "    finally:\n",
    "        controller.close()\n",
    "\n",
    "    append_rollouts(records, rollouts_path=rollouts_path)\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dacc9aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_rollouts(records: Sequence[dict]) -> pd.DataFrame:\n",
    "    \"\"\"Create a quick per-variant summary of response counts and lengths.\"\"\"\n",
    "\n",
    "    if not records:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.DataFrame(records)\n",
    "    df[\"response_length\"] = df[\"response\"].fillna(\"\").map(len)\n",
    "    summary = (\n",
    "        df.groupby([\"steering_source\", \"variant\"])\n",
    "        .agg(\n",
    "            responses=(\"response\", \"count\"),\n",
    "            avg_response_length=(\"response_length\", \"mean\"),\n",
    "            avg_scale=(\"steering_scale\", \"mean\"),\n",
    "        )\n",
    "        .sort_index()\n",
    "    )\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad0fc89",
   "metadata": {},
   "source": [
    "## Train a steering vector (optional)\n",
    "Adjust `DATASET_NAME` and override defaults (e.g. `target_tokens`) before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26c047a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training steering vector for qwen-3-32b__trait__acerbic. This may take significant time.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'build_training_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DATASET_NAME:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining steering vector for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. This may take significant time.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     training_result = \u001b[43mtrain_steering_vector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mDATASET_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Example override: target_tokens=10_000,\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Example override: max_steps=200,\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     display(training_result)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain_steering_vector\u001b[39m\u001b[34m(dataset_name, output_root, run_validation, **overrides)\u001b[39m\n\u001b[32m     10\u001b[39m output_dir = Path(output_root) / dataset_name\n\u001b[32m     11\u001b[39m output_dir.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m args = \u001b[43mbuild_training_args\u001b[49m(dataset_name, output_dir, **overrides)\n\u001b[32m     15\u001b[39m model, tokenizer = get_training_components(\n\u001b[32m     16\u001b[39m     model_name=args.model,\n\u001b[32m     17\u001b[39m     target_layer=args.target_layer,\n\u001b[32m     18\u001b[39m     device_map=args.device_map,\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m reset_shared_vector(model, args.init_scale)\n",
      "\u001b[31mNameError\u001b[39m: name 'build_training_args' is not defined"
     ]
    }
   ],
   "source": [
    "if DATASET_NAME:\n",
    "    print(f\"Training steering vector for {DATASET_NAME}. This may take significant time.\")\n",
    "    training_result = train_steering_vector(\n",
    "        DATASET_NAME,\n",
    "        # Example override: target_tokens=10_000,\n",
    "        # Example override: max_steps=200,\n",
    "    )\n",
    "    display(training_result)\n",
    "else:\n",
    "    print(\"Set DATASET_NAME to a {role,trait} dataset string before training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59490423",
   "metadata": {},
   "source": [
    "## Load existing vectors for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba70fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_NAME:\n",
    "    trained_info = load_trained_vector(DATASET_NAME)\n",
    "    activation_info = load_activation_vector(DATASET_NAME)\n",
    "    display({\n",
    "        \"trained_layer\": trained_info[\"layer\"],\n",
    "        \"trained_norm\": trained_info[\"norm\"],\n",
    "        \"trained_path\": str(trained_info[\"path\"]),\n",
    "        \"activation_layer\": activation_info[\"layer\"],\n",
    "        \"activation_norm\": activation_info[\"norm\"],\n",
    "        \"activation_path\": str(activation_info[\"path\"]) if activation_info[\"path\"] else None,\n",
    "    })\n",
    "else:\n",
    "    print(\"Set DATASET_NAME before loading vectors.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e588394",
   "metadata": {},
   "source": [
    "## Generate and evaluate rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56345f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_NAME:\n",
    "    records = generate_dataset_rollouts(\n",
    "        DATASET_NAME,\n",
    "        num_rollouts=1,\n",
    "        trained_scales=(1.0,),\n",
    "        activation_scales=(1.0,),\n",
    "        # Example: override generation config\n",
    "        # gen_config=RolloutGenerationConfig(max_new_tokens=128, temperature=0.8),\n",
    "    )\n",
    "    summary = summarize_rollouts(records)\n",
    "    display(summary)\n",
    "else:\n",
    "    print(\"Set DATASET_NAME before running rollouts.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
