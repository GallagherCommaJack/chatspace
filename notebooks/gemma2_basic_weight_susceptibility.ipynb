{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma2-27B Basic Weight Susceptibility Analysis\n",
    "\n",
    "This notebook analyzes how PC vectors from persona subspace data activate downstream neurons differently between Gemma2-27B base and instruct models.\n",
    "\n",
    "**Key Questions:**\n",
    "1. Do PC vectors (especially PC1) show different activation patterns in base vs instruct models?\n",
    "2. Which layers show the strongest weight differences?\n",
    "3. How do PC vectors compare to random baselines?\n",
    "\n",
    "**Approach:**\n",
    "- Load both base and instruct models\n",
    "- Compute weight differences\n",
    "- Load PC vectors from PCA data\n",
    "- Apply layernorm and project through weight matrices\n",
    "- Compute cosine distances between base and instruct activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "# Import from chatspace.analysis\n",
    "from chatspace.analysis import (\n",
    "    load_pca_data,\n    load_individual_role_vectors,\n    load_individual_trait_vectors,\n",
    "    extract_pc_components,\n",
    "    normalize_vector,\n",
    "    gemma2_rmsnorm,\n",
    "    compute_cosine_distances_batch,\n",
    "    compute_weight_statistics\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Gemma2-27B Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model identifiers\n",
    "base_model_id = \"google/gemma-2-27b\"\n",
    "instruct_model_id = \"google/gemma-2-27b-it\"\n",
    "\n",
    "print(f\"Loading models...\")\n",
    "print(f\"  Base: {base_model_id}\")\n",
    "print(f\"  Instruct: {instruct_model_id}\")\n",
    "print(f\"\\nThis will take several minutes...\\n\")\n",
    "\n",
    "# Load config\n",
    "config = AutoConfig.from_pretrained(base_model_id)\n",
    "print(f\"Model config:\")\n",
    "print(f\"  Hidden size: {config.hidden_size}\")\n",
    "print(f\"  Num layers: {config.num_hidden_layers}\")\n",
    "print(f\"  Intermediate size: {config.intermediate_size}\")\n",
    "print(f\"  Attention heads: {config.num_attention_heads}\")\n",
    "print(f\"  KV heads: {config.num_key_value_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "base_state_dict = {k: v.cpu() for k, v in base_model.state_dict().items()}\n",
    "print(f\"\u2713 Base model loaded: {len(base_state_dict)} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load instruct model\n",
    "print(\"Loading instruct model...\")\n",
    "instruct_model = AutoModelForCausalLM.from_pretrained(\n",
    "    instruct_model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "instruct_state_dict = {k: v.cpu() for k, v in instruct_model.state_dict().items()}\n",
    "print(f\"\u2713 Instruct model loaded: {len(instruct_state_dict)} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Weight Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weight differences\n",
    "print(\"Computing weight differences...\")\n",
    "weight_diffs = {}\n",
    "for name in base_state_dict.keys():\n",
    "    if name in instruct_state_dict:\n",
    "        diff = instruct_state_dict[name] - base_state_dict[name]\n",
    "        weight_diffs[name] = diff\n",
    "\n",
    "print(f\"\\nComputed {len(weight_diffs)} weight differences\")\n",
    "print(f\"\\nSample statistics:\")\n",
    "for i, (name, diff) in enumerate(list(weight_diffs.items())[:5]):\n",
    "    print(f\"  {name}: shape={diff.shape}, norm={diff.float().norm().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-load-intro",
   "metadata": {},
   "source": "## 3. Load PC Vectors and Semantic Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load-all",
   "metadata": {},
   "outputs": [],
   "source": "# Path to persona subspace data\npersona_data_root = Path(\"/workspace/persona-data\")\ngemma_model_name = \"gemma-2-27b\"\n\nroles_pca_dir = persona_data_root / gemma_model_name / \"roles_240\" / \"pca\"\ntraits_pca_dir = persona_data_root / gemma_model_name / \"traits_240\" / \"pca\"\nroles_vectors_dir = persona_data_root / gemma_model_name / \"roles_240\" / \"vectors\"\ntraits_vectors_dir = persona_data_root / gemma_model_name / \"traits_240\" / \"vectors\"\n\nprint(f\"Checking data paths...\")\nprint(f\"  Roles PCA: {roles_pca_dir.exists()}\")\nprint(f\"  Traits PCA: {traits_pca_dir.exists()}\")\n\n# Load PCA data\npca_data, all_pca_files = load_pca_data(roles_pca_dir)\npca_layer = pca_data['layer']\nprint(f\"\\n\u2713 Loaded PCA data from layer {pca_layer}\")\n\n# Extract ALL PCs (load 10 for flexibility)\nn_pcs_total = 10\npcs_all, variance_all = extract_pc_components(pca_data, n_components=n_pcs_total)\nprint(f\"  Extracted {n_pcs_total} PCs\")\nprint(f\"  Variance explained by first 5: {variance_all[:5]}\")\n\n# Load individual role and trait vectors at PCA layer\n# Uses discriminative defaults:\n# - Roles: pos_3 - default_1 (difference vectors)\n# - Traits: pos_neg_50 (precomputed contrast vectors)\nrole_vectors = load_individual_role_vectors(roles_vectors_dir, pca_layer)\ntrait_vectors = load_individual_trait_vectors(traits_vectors_dir, pca_layer)\n\nprint(f\"\\n\u2713 Loaded semantic vectors:\")\nprint(f\"  {len(role_vectors)} role difference vectors\")\nprint(f\"  {len(trait_vectors)} trait contrast vectors\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-config",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# CONFIGURATION: Set analysis parameters here\n# ============================================================================\n\n# Which PCs to visualize in plots\nplot_pcs = [\"PC1\", \"PC2\", \"PC3\"]\n# plot_pcs = [\"PC1\"]  # Uncomment to focus on PC1 only\n\n# How many layers before/after PCA layer to analyze\nn_layers_context = 5\n\n# How many random baseline vectors\nn_random_baseline = 20\n\n# How many role/trait vectors to sample\nn_sample_roles = 5\nn_sample_traits = 5\n\nprint(f\"\ud83d\udccb Analysis Configuration:\")\nprint(f\"  Analyzing {len(plot_pcs)} PCs: {plot_pcs}\")\nprint(f\"  Context: \u00b1{n_layers_context} layers around layer {pca_layer}\")\nprint(f\"  Random baseline: {n_random_baseline} vectors\")\nprint(f\"  Sampling {n_sample_roles} roles + {n_sample_traits} traits\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-compute-intro",
   "metadata": {},
   "source": "## 4. Extract Weights and Compute Cosine Distances"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-extract-weights",
   "metadata": {},
   "outputs": [],
   "source": "# Define layers to analyze (based on config)\nupstream_layers = range(max(0, pca_layer - n_layers_context), pca_layer)\ndownstream_layers = range(pca_layer + 1, min(pca_layer + n_layers_context + 1, config.num_hidden_layers))\nanalysis_layers = list(upstream_layers) + list(downstream_layers)\n\nprint(f\"Analyzing layers (\u00b1{n_layers_context} from layer {pca_layer}):\")\nprint(f\"  Upstream: {list(upstream_layers)}\")\nprint(f\"  Downstream: {list(downstream_layers)}\")\n\n# Extract relevant weights\ntarget_weight_types = ['up_proj.weight', 'gate_proj.weight', 'q_proj.weight', 'k_proj.weight', 'v_proj.weight']\nrelevant_weights = {}\n\nfor name, diff in weight_diffs.items():\n    for layer_num in analysis_layers:\n        layer_prefix = f\"model.layers.{layer_num}.\"\n        if name.startswith(layer_prefix):\n            for weight_type in target_weight_types:\n                if name.endswith(weight_type):\n                    relevant_weights[name] = diff\n                    break\n\nprint(f\"\\n\u2713 Found {len(relevant_weights)} relevant weight matrices\")\n\n# Extract layernorm weights\nlayernorm_weights = {}\nfor layer_num in analysis_layers:\n    input_ln_name = f\"model.layers.{layer_num}.input_layernorm.weight\"\n    pre_ffn_ln_name = f\"model.layers.{layer_num}.pre_feedforward_layernorm.weight\"\n    \n    if input_ln_name in base_state_dict:\n        layernorm_weights[f\"{layer_num}.input_layernorm\"] = {\n            'base': base_state_dict[input_ln_name],\n            'instruct': instruct_state_dict[input_ln_name]\n        }\n    \n    if pre_ffn_ln_name in base_state_dict:\n        layernorm_weights[f\"{layer_num}.pre_feedforward_layernorm\"] = {\n            'base': base_state_dict[pre_ffn_ln_name],\n            'instruct': instruct_state_dict[pre_ffn_ln_name]\n        }\n\nprint(f\"\u2713 Extracted {len(layernorm_weights)} layernorm weights\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-build-vectors",
   "metadata": {},
   "outputs": [],
   "source": "# Build test vector batch\nall_vectors = []\nvector_names = []\n\n# Add configured PCs\nfor pc_name in plot_pcs:\n    pc_idx = int(pc_name.replace(\"PC\", \"\")) - 1\n    all_vectors.append(pcs_all[pc_idx])\n    vector_names.append(pc_name)\n\n# Sample role vectors\nsample_roles = list(role_vectors.items())[:n_sample_roles]\nfor name, vec in sample_roles:\n    all_vectors.append(vec)\n    vector_names.append(f\"role:{name}\")\n\n# Sample trait vectors\nsample_traits = list(trait_vectors.items())[:n_sample_traits]\nfor name, vec in sample_traits:\n    all_vectors.append(vec)\n    vector_names.append(f\"trait:{name}\")\n\n# Add random baseline\ntorch.manual_seed(42)\nfor i in range(n_random_baseline):\n    rand_vec = torch.randn(config.hidden_size, dtype=torch.float32)\n    rand_vec = normalize_vector(rand_vec).to(torch.bfloat16)\n    all_vectors.append(rand_vec)\n    vector_names.append(f\"Random{i+1}\")\n\n# Stack into batch\nvectors_batch = torch.stack(all_vectors)\nprint(f\"\u2713 Prepared {len(vector_names)} test vectors:\")\nprint(f\"  {len(plot_pcs)} PCs: {plot_pcs}\")\nprint(f\"  {len(sample_roles)} roles\")\nprint(f\"  {len(sample_traits)} traits\")\nprint(f\"  {n_random_baseline} random baseline\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-compute-distances",
   "metadata": {},
   "outputs": [],
   "source": "# Compute cosine distances for each weight matrix\nresults = []\n\nprint(f\"Computing cosine distances for {len(relevant_weights)} weight matrices...\")\n\nfor weight_name, weight_diff in tqdm(relevant_weights.items(), desc=\"Processing weights\"):\n    # Parse layer and weight type\n    parts = weight_name.split('.')\n    layer_num = int(parts[2])\n    weight_type = parts[-2]  # e.g., 'up_proj', 'gate_proj', etc.\n    \n    # Get corresponding weights from base and instruct\n    base_weight = base_state_dict[weight_name]\n    instruct_weight = instruct_state_dict[weight_name]\n    \n    # Determine appropriate layernorm\n    if weight_type in ['up_proj', 'gate_proj']:\n        ln_key = f\"{layer_num}.pre_feedforward_layernorm\"\n    else:  # attention weights\n        ln_key = f\"{layer_num}.input_layernorm\"\n    \n    if ln_key not in layernorm_weights:\n        continue\n    \n    ln_base = layernorm_weights[ln_key]['base']\n    ln_instruct = layernorm_weights[ln_key]['instruct']\n    \n    # Apply layernorm and compute projections\n    normed_base = gemma2_rmsnorm(vectors_batch, ln_base)\n    normed_instruct = gemma2_rmsnorm(vectors_batch, ln_instruct)\n    \n    proj_base = normed_base @ base_weight.T\n    proj_instruct = normed_instruct @ instruct_weight.T\n    \n    # Compute cosine distances\n    cosine_dists = compute_cosine_distances_batch(proj_base, proj_instruct)\n    \n    # Store results\n    for i, vec_name in enumerate(vector_names):\n        results.append({\n            'vector': vec_name,\n            'layer': layer_num,\n            'weight_type': weight_type,\n            'cosine_distance': float(cosine_dists[i])\n        })\n\nresults_df = pd.DataFrame(results)\nprint(f\"\\n\u2713 Computed {len(results_df)} cosine distance measurements\")\nprint(f\"  Shape: {results_df.shape}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-viz-intro",
   "metadata": {},
   "source": "## 5. Visualize Results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-summary-stats",
   "metadata": {},
   "outputs": [],
   "source": "# Summary statistics\nprint(\"=\"*80)\nprint(\"SUMMARY STATISTICS\")\nprint(\"=\"*80)\n\n# Mean cosine distance by vector type\nsummary = results_df.groupby('vector')['cosine_distance'].agg(['mean', 'std', 'min', 'max'])\nsummary = summary.sort_values('mean', ascending=False)\n\nprint(\"\\nMean cosine distance by vector (top 10):\")\nprint(summary.head(10))\n\n# Compare configured PCs to random baseline\npc_means = {pc: results_df[results_df['vector'] == pc]['cosine_distance'].mean() for pc in plot_pcs}\nrandom_mean = results_df[results_df['vector'].str.startswith('Random')]['cosine_distance'].mean()\n\nprint(f\"\\nPC vs Random baseline comparison:\")\nprint(f\"  Random baseline mean: {random_mean:.6f}\")\nfor pc in plot_pcs:\n    pc_mean = pc_means[pc]\n    ratio = pc_mean / random_mean\n    print(f\"  {pc} mean: {pc_mean:.6f} ({ratio:.2f}x random)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-viz-plots",
   "metadata": {},
   "outputs": [],
   "source": "# Plot: Cosine distance by weight type and layer\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# By weight type (configured PCs only)\npc_data = results_df[results_df['vector'].isin(plot_pcs)]\nsns.boxplot(data=pc_data, x='weight_type', y='cosine_distance', ax=axes[0])\naxes[0].set_title(f'Cosine Distance by Weight Type ({\", \".join(plot_pcs)})')\naxes[0].set_ylabel('Cosine Distance')\naxes[0].tick_params(axis='x', rotation=45)\naxes[0].grid(True, alpha=0.3, axis='y')\n\n# By layer (line plot for each configured PC)\npc_data_mean = pc_data.groupby(['layer', 'vector'])['cosine_distance'].mean().reset_index()\nfor pc in plot_pcs:\n    data = pc_data_mean[pc_data_mean['vector'] == pc]\n    axes[1].plot(data['layer'], data['cosine_distance'], marker='o', label=pc, linewidth=2)\n\naxes[1].axvline(pca_layer, color='red', linestyle=':', alpha=0.5, label=f'PCA layer ({pca_layer})')\naxes[1].set_title('Mean Cosine Distance by Layer')\naxes[1].set_xlabel('Layer')\naxes[1].set_ylabel('Cosine Distance')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-viz-heatmap",
   "metadata": {},
   "outputs": [],
   "source": "# Heatmap: Cosine distances for configured PCs across layers and weight types\npivot_data = pc_data.pivot_table(\n    index='weight_type',\n    columns='layer',\n    values='cosine_distance',\n    aggfunc='mean'\n)\n\nfig, ax = plt.subplots(figsize=(12, 5))\nsns.heatmap(pivot_data, annot=True, fmt='.4f', cmap='YlOrRd', ax=ax, cbar_kws={'label': 'Cosine Distance'})\nax.set_title(f'Mean Cosine Distance: {\", \".join(plot_pcs)} across Layers and Weight Types')\nax.set_xlabel('Layer')\nax.set_ylabel('Weight Type')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary",
   "metadata": {},
   "source": "## 6. Key Findings\n\nThis analysis reveals which layers and weight types show the strongest differences between base and instruct models when processing PC vectors from persona subspace.\n\n**Interpretation:**\n- **Higher cosine distances** = Instruction tuning significantly altered how that weight responds to the semantic direction\n- **Layer patterns** = Shows where in the network instruction tuning has strongest effect\n- **Weight type differences** = Reveals whether MLP (gate/up) or attention (q/k/v) is more affected\n- **PC vs Random** = Measures whether PCs are specifically targeted vs general weight changes"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}