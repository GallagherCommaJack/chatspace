{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Steering Vector Training and Evaluation\n",
    "\n",
    "This notebook provides a complete pipeline for:\n",
    "1. Loading base models\n",
    "2. Training steering vectors for persona datasets (traits/roles)\n",
    "3. Loading pre-trained steering vectors\n",
    "4. Loading activation-based steering vectors\n",
    "5. Evaluating steering vectors with rollout generation\n",
    "6. Saving rollouts with metadata to JSONL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from dataclasses import asdict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# Import from chatspace steering modules\n",
    "import sys\n",
    "sys.path.insert(0, '/root/chatspace')\n",
    "from chatspace.steering.train import build_trainer, build_argparser\n",
    "from chatspace.steering.model import QwenSteerModel, SteeringVectorConfig\n",
    "from chatspace.steering.data import PersonaSteeringDatasetConfig, load_persona_steering_dataset\n",
    "\n",
    "# Configuration\n",
    "PERSONA_ROOT = Path(\"/workspace/persona-data\")\n",
    "STEERING_RUN_ROOT = Path(\"/workspace/steering_runs_qwen3_layer_30\")\n",
    "ROLLOUT_OUTPUT_ROOT = Path(\"/workspace/steering_rollouts_qwen3_layer_30\")\n",
    "TARGET_LAYER = 30  # Default layer for Qwen3-32B\n",
    "BASE_MODEL = \"Qwen/Qwen3-32B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_loading_header",
   "metadata": {},
   "source": [
    "## 1. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_base_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: Qwen/Qwen3-32B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd8973a92bf46308e76d89bd6a05e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "def load_base_model(model_name: str = BASE_MODEL, device_map: str = \"auto\"):\n",
    "    \"\"\"Load the base causal LM and tokenizer.\"\"\"\n",
    "    print(f\"Loading base model: {model_name}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=device_map,\n",
    "        low_cpu_mem_usage=False,\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded on device: {next(model.parameters()).device}\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# Load the model\n",
    "base_model, tokenizer = load_base_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steering_controller_header",
   "metadata": {},
   "source": [
    "## 2. Steering Controller\n",
    "\n",
    "This controller allows us to dynamically swap steering vectors during generation.\n",
    "\n",
    "**Note**: We use two separate model instances:\n",
    "- `base_model`: Plain AutoModelForCausalLM for evaluation and rollout generation\n",
    "- `steering_model`: QwenSteerModel (loaded in section 3) for training steering vectors\n",
    "\n",
    "The controller is attached to `base_model` for efficient vector swapping during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "steering_controller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created steering controller for base_model\n"
     ]
    }
   ],
   "source": [
    "class SteeringController:\n",
    "    \"\"\"Attach a single residual hook and swap steering vectors on demand.\"\"\"\n",
    "\n",
    "    def __init__(self, model: AutoModelForCausalLM) -> None:\n",
    "        self.model = model\n",
    "        self.layer_idx: int | None = None\n",
    "        self._handle = None\n",
    "        self.vector: torch.Tensor | None = None\n",
    "\n",
    "    def _hook(self, module, args, output):\n",
    "        if self.vector is None:\n",
    "            return output\n",
    "        hidden = output[0] if isinstance(output, tuple) else output\n",
    "        vec = self.vector\n",
    "        if vec.device != hidden.device or vec.dtype != hidden.dtype:\n",
    "            vec = vec.to(device=hidden.device, dtype=hidden.dtype)\n",
    "            self.vector = vec\n",
    "        steered = hidden + vec\n",
    "        if isinstance(output, tuple):\n",
    "            return (steered,) + output[1:]\n",
    "        return steered\n",
    "\n",
    "    def set_layer(self, layer_idx: int) -> None:\n",
    "        if self.layer_idx == layer_idx:\n",
    "            return\n",
    "        if self._handle is not None:\n",
    "            self._handle.remove()\n",
    "        layer = self.model.model.layers[layer_idx]\n",
    "        self._handle = layer.register_forward_hook(self._hook)\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "    def set_vector(self, vector: torch.Tensor | None) -> None:\n",
    "        if vector is None:\n",
    "            self.vector = None\n",
    "            return\n",
    "        if vector.ndim != 1:\n",
    "            raise ValueError(\"Steering vector must be 1D\")\n",
    "        self.vector = vector\n",
    "\n",
    "    def close(self) -> None:\n",
    "        if self._handle is not None:\n",
    "            self._handle.remove()\n",
    "            self._handle = None\n",
    "\n",
    "# Create steering controller for base_model (used for evaluation/rollouts)\n",
    "controller = SteeringController(base_model)\n",
    "print(\"Created steering controller for base_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_function_header",
   "metadata": {},
   "source": [
    "## 3. Load Reusable Steering Model\n",
    "\n",
    "Load QwenSteerModel once and reuse it for training multiple datasets (following train_all_steering.py pattern)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "train_steering_vector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating steering model wrapper around existing base model\n",
      "Target layer: 30\n",
      "Steering model created (sharing weights with base_model)\n",
      "Steering vector on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "def load_steering_model(\n",
    "    base_model: AutoModelForCausalLM,\n",
    "    target_layer: int = TARGET_LAYER,\n",
    "    init_scale: float = 0.0,\n",
    ") -> QwenSteerModel:\n",
    "    \"\"\"Load a QwenSteerModel that wraps an existing base model for weight reuse.\n",
    "\n",
    "    Args:\n",
    "        base_model: Existing AutoModelForCausalLM to wrap\n",
    "        target_layer: Layer to apply steering\n",
    "        init_scale: Initial scale for steering vector\n",
    "\n",
    "    Returns:\n",
    "        QwenSteerModel instance that shares weights with base_model\n",
    "    \"\"\"\n",
    "    print(f\"Creating steering model wrapper around existing base model\")\n",
    "    print(f\"Target layer: {target_layer}\")\n",
    "\n",
    "    # Create config\n",
    "    model_cfg = SteeringVectorConfig(\n",
    "        model_name=BASE_MODEL,\n",
    "        target_layer=target_layer,\n",
    "        init_scale=init_scale,\n",
    "    )\n",
    "\n",
    "    # Create QwenSteerModel instance without calling __init__\n",
    "    # This avoids loading the weights twice\n",
    "    steering_model = QwenSteerModel.__new__(QwenSteerModel)\n",
    "    nn.Module.__init__(steering_model)  # Initialize as nn.Module\n",
    "    \n",
    "    # Set config\n",
    "    steering_model.cfg = model_cfg\n",
    "    \n",
    "    # Share the base model (this is the key - we don't load new weights)\n",
    "    steering_model.model = base_model\n",
    "    steering_model.config = base_model.config\n",
    "    \n",
    "    # Freeze base model parameters\n",
    "    for param in steering_model.model.parameters():\n",
    "        param.requires_grad_(False)\n",
    "    \n",
    "    # Initialize the steering hook\n",
    "    from chatspace.steering.model import ResidualHook\n",
    "    hidden_size = base_model.config.hidden_size\n",
    "    steering_model.steering = ResidualHook(hidden_size, init_scale)\n",
    "    steering_model._hook_handle = None\n",
    "    \n",
    "    # Install the forward hook\n",
    "    steering_model._install_hook()\n",
    "    \n",
    "    # Move steering vector to same device as base model\n",
    "    device = next(base_model.parameters()).device\n",
    "    steering_model.steering = steering_model.steering.to(device)\n",
    "    \n",
    "    print(f\"Steering model created (sharing weights with base_model)\")\n",
    "    print(f\"Steering vector on device: {steering_model.steering.vector.device}\")\n",
    "    \n",
    "    return steering_model\n",
    "\n",
    "def reset_steering_vector(model: QwenSteerModel, init_scale: float = 0.0) -> None:\n",
    "    \"\"\"Reset the steering vector to initial state between training runs.\n",
    "\n",
    "    Args:\n",
    "        model: QwenSteerModel instance\n",
    "        init_scale: Initialization scale (0.0 for zeros)\n",
    "    \"\"\"\n",
    "    if init_scale == 0.0:\n",
    "        model.steering.vector.data.zero_()\n",
    "    else:\n",
    "        torch.nn.init.normal_(model.steering.vector, mean=0.0, std=init_scale)\n",
    "    if model.steering.vector.grad is not None:\n",
    "        model.steering.vector.grad.zero_()\n",
    "\n",
    "# Load steering model (wraps base_model, shares weights)\n",
    "steering_model = load_steering_model(\n",
    "    base_model=base_model,\n",
    "    target_layer=TARGET_LAYER,\n",
    "    init_scale=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7itu802t9e4",
   "metadata": {},
   "source": [
    "## 4. Train Steering Vector\n",
    "\n",
    "Function to train a steering vector for a given dataset using TRL's SFTTrainer with model reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bol943jk3s6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_split(\n",
    "    dataset_name: str,\n",
    "    tokenizer,\n",
    "    train_tokens: int = 100_000,\n",
    "    val_tokens: int = 10_000,\n",
    "    seed: int = 17,\n",
    "    role_score: int = 3,\n",
    "    trait_score: int = 75,\n",
    "    max_length: int = 4096,\n",
    "):\n",
    "    \"\"\"Prepare train/val split for a dataset.\"\"\"\n",
    "    from chatspace.steering.data import PersonaSteeringDatasetConfig, load_persona_steering_dataset\n",
    "    \n",
    "    cfg = PersonaSteeringDatasetConfig(\n",
    "        dataset_names=[dataset_name],\n",
    "        target_tokens=train_tokens + max(val_tokens, 0),\n",
    "        seed=seed,\n",
    "        tokenizer_name=BASE_MODEL,\n",
    "        max_length=max_length,\n",
    "        role_min_score=role_score,\n",
    "        trait_min_score=trait_score,\n",
    "    )\n",
    "    full_dataset = load_persona_steering_dataset(cfg, tokenizer)\n",
    "\n",
    "    token_lengths = list(full_dataset[\"length\"])\n",
    "    cumulative = 0\n",
    "    train_idx: list[int] = []\n",
    "    val_idx: list[int] = []\n",
    "\n",
    "    for idx, length in enumerate(token_lengths):\n",
    "        cumulative += int(length)\n",
    "        if cumulative <= train_tokens:\n",
    "            train_idx.append(idx)\n",
    "        elif val_tokens > 0 and cumulative <= train_tokens + val_tokens:\n",
    "            val_idx.append(idx)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if not train_idx:\n",
    "        raise ValueError(\"No training examples selected; increase tokens or relax score filters\")\n",
    "\n",
    "    train_dataset = full_dataset.select(train_idx)\n",
    "    train_tokens_actual = sum(int(full_dataset[i][\"length\"]) for i in train_idx)\n",
    "\n",
    "    val_dataset = None\n",
    "    val_tokens_actual = 0\n",
    "    if val_idx:\n",
    "        val_dataset = full_dataset.select(val_idx)\n",
    "        val_tokens_actual = sum(int(full_dataset[i][\"length\"]) for i in val_idx)\n",
    "\n",
    "    print(\n",
    "        f\"Prepared {dataset_name}: {len(train_dataset)} train seq / {train_tokens_actual} tokens\"\n",
    "        + (f\"; val {len(val_dataset)} seq / {val_tokens_actual} tokens.\" if val_dataset is not None else \".\")\n",
    "    )\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def train_steering_vector(\n",
    "    dataset_name: str,\n",
    "    model: QwenSteerModel,\n",
    "    tokenizer,\n",
    "    output_dir: Path | None = None,\n",
    "    learning_rate: float = 5e-1,\n",
    "    init_scale: float = 0.0,\n",
    "    batch_size: int = 4,\n",
    "    gradient_accumulation: int = 1,\n",
    "    train_tokens: int = 100_000,\n",
    "    val_tokens: int = 10_000,\n",
    "    num_epochs: float = 5.0,\n",
    "    seed: int = 17,\n",
    "    role_score: int = 3,\n",
    "    trait_score: int = 75,\n",
    "    bf16: bool = True,\n",
    "    early_stop_patience: int = 3,\n",
    "    early_stop_threshold: float = 0.0,\n",
    "    eval_steps: int = 200,\n",
    "    logging_steps: int = 50,\n",
    "    compare_prompted: bool = False,\n",
    "    lr_scheduler: str = \"cosine\",\n",
    "    warmup_ratio: float = 0.05,\n",
    ") -> Path:\n",
    "    \"\"\"Train a steering vector for the given dataset by reusing existing model.\n",
    "    \n",
    "    This function follows train_all_steering.py pattern: it reuses the model and\n",
    "    resets the steering vector between runs.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of the persona dataset (e.g., 'qwen-3-32b__trait__acerbic')\n",
    "        model: QwenSteerModel instance to reuse\n",
    "        tokenizer: Tokenizer instance\n",
    "        output_dir: Output directory for trained vector (default: STEERING_RUN_ROOT / dataset_name)\n",
    "        learning_rate: Learning rate for steering vector\n",
    "        init_scale: Initialization scale for steering vector\n",
    "        batch_size: Per-device batch size\n",
    "        gradient_accumulation: Gradient accumulation steps\n",
    "        train_tokens: Total training tokens\n",
    "        val_tokens: Validation tokens (0 disables validation)\n",
    "        num_epochs: Number of training epochs\n",
    "        seed: Random seed\n",
    "        role_score: Minimum role extract_score\n",
    "        trait_score: Minimum trait extract_score\n",
    "        bf16: Enable bfloat16 training\n",
    "        early_stop_patience: Early stopping patience (0 disables)\n",
    "        early_stop_threshold: Minimum improvement threshold\n",
    "        eval_steps: Evaluation frequency in steps\n",
    "        logging_steps: Logging frequency in steps\n",
    "        compare_prompted: Compare against prompted baseline\n",
    "        lr_scheduler: Learning rate scheduler type\n",
    "        warmup_ratio: Warmup ratio for scheduler\n",
    "    \n",
    "    Returns:\n",
    "        Path to output directory containing steering_vector.pt and steering_config.json\n",
    "    \"\"\"\n",
    "    from chatspace.steering.train import EarlyStopCallback, _compute_average_loss\n",
    "    from trl.trainer.sft_trainer import SFTConfig, SFTTrainer\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = STEERING_RUN_ROOT / dataset_name\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n=== Training {dataset_name} ===\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print(f\"LR: {learning_rate}, Epochs: {num_epochs}, Tokens: {train_tokens}\")\n",
    "    \n",
    "    # Prepare dataset split\n",
    "    train_dataset, val_dataset = prepare_dataset_split(\n",
    "        dataset_name, tokenizer, train_tokens, val_tokens, seed, role_score, trait_score\n",
    "    )\n",
    "    \n",
    "    # Reset steering vector\n",
    "    reset_steering_vector(model, init_scale)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    eval_strategy = \"steps\" if val_dataset is not None else \"no\"\n",
    "    \n",
    "    sft_config = SFTConfig(\n",
    "        output_dir=str(output_dir),\n",
    "        seed=seed,\n",
    "        do_eval=val_dataset is not None,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation,\n",
    "        max_steps=-1,\n",
    "        bf16=bf16 and torch.cuda.is_available(),\n",
    "        num_train_epochs=num_epochs,\n",
    "        logging_steps=max(1, logging_steps),\n",
    "        eval_strategy=eval_strategy,\n",
    "        eval_steps=max(1, eval_steps),\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        report_to=[],\n",
    "        gradient_checkpointing=False,\n",
    "        lr_scheduler_type=lr_scheduler,\n",
    "        save_strategy=\"no\",\n",
    "        save_only_model=True,\n",
    "        save_total_limit=1,\n",
    "    )\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=sft_config,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "    \n",
    "    trainer.create_model_card = lambda *_, **__: None\n",
    "    \n",
    "    def _save_model(target: str | None = None, _internal_call: bool = False) -> None:\n",
    "        dest = Path(target) if target is not None else output_dir\n",
    "        model.save_pretrained(dest)\n",
    "    \n",
    "    trainer.save_model = _save_model  # type: ignore[assignment]\n",
    "    \n",
    "    early_cb = None\n",
    "    if val_dataset is not None and early_stop_patience > 0:\n",
    "        early_cb = EarlyStopCallback(trainer, early_stop_patience, early_stop_threshold)\n",
    "        trainer.add_callback(early_cb)\n",
    "    \n",
    "    metrics: dict[str, float | str] = {\"dataset\": dataset_name, \"learning_rate\": learning_rate}\n",
    "    \n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "    except ValueError as exc:\n",
    "        print(f\"Failed on {dataset_name}: {exc}\")\n",
    "        return output_dir\n",
    "    \n",
    "    metrics.update({\n",
    "        \"train_runtime\": train_result.metrics.get(\"train_runtime\"),\n",
    "        \"train_loss\": train_result.metrics.get(\"train_loss\"),\n",
    "        \"epoch\": train_result.metrics.get(\"epoch\"),\n",
    "    })\n",
    "    \n",
    "    # Restore best vector from early stopping\n",
    "    if early_cb is not None and getattr(early_cb, \"best_vector\", None) is not None:\n",
    "        best_vec = early_cb.best_vector.to(model.steering.vector.device)\n",
    "        model.steering.vector.data.copy_(best_vec)\n",
    "    \n",
    "    # Evaluate if validation set exists\n",
    "    if val_dataset is not None:\n",
    "        eval_metrics = trainer.evaluate()\n",
    "        eval_loss = eval_metrics.get(\"eval_loss\")\n",
    "        if eval_loss is None:\n",
    "            eval_loss = _compute_average_loss(model, trainer.get_eval_dataloader())\n",
    "            eval_metrics[\"eval_loss\"] = eval_loss\n",
    "        eval_metrics[\"eval_ppl\"] = math.exp(eval_loss)\n",
    "        metrics.update(eval_metrics)\n",
    "        print(\"Validation metrics:\", eval_metrics)\n",
    "        \n",
    "        if compare_prompted:\n",
    "            stored_vec = model.steering.vector.detach().clone()\n",
    "            model.steering.vector.data.zero_()\n",
    "            base_loss = _compute_average_loss(model, trainer.get_eval_dataloader())\n",
    "            metrics[\"baseline_loss\"] = base_loss\n",
    "            metrics[\"baseline_ppl\"] = math.exp(base_loss)\n",
    "            model.steering.vector.data.copy_(stored_vec)\n",
    "    \n",
    "    # Save model and metrics\n",
    "    model.save_pretrained(output_dir)\n",
    "    (output_dir / \"metrics.json\").write_text(json.dumps(metrics, indent=2))\n",
    "    \n",
    "    # Cleanup\n",
    "    if hasattr(trainer, \"accelerator\"):\n",
    "        trainer.accelerator.free_memory()\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"✓ Training complete. Saved to {output_dir}\")\n",
    "    return output_dir\n",
    "\n",
    "# Example usage (commented out - uncomment to train)\n",
    "# trained_dir = train_steering_vector(\n",
    "#     dataset_name=\"qwen-3-32b__trait__acerbic\",\n",
    "#     model=steering_model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     train_tokens=100_000,\n",
    "#     val_tokens=10_000,\n",
    "#     num_epochs=5,\n",
    "#     early_stop_patience=3,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_trained_header",
   "metadata": {},
   "source": [
    "## 5. Load Trained Steering Vector\n",
    "\n",
    "Load a previously trained steering vector from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "load_trained_vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_steering_vector(\n",
    "    dataset: str,\n",
    "    run_root: Path = STEERING_RUN_ROOT,\n",
    ") -> tuple[torch.Tensor, int] | None:\n",
    "    \"\"\"Load trained steering vector for a given dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset name (e.g., 'qwen-3-32b__trait__acerbic')\n",
    "        run_root: Root directory containing trained vectors\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (vector, target_layer) or None if not found\n",
    "    \"\"\"\n",
    "    steering_dir = run_root / dataset\n",
    "    vector_path = steering_dir / \"steering_vector.pt\"\n",
    "    \n",
    "    if not vector_path.exists():\n",
    "        print(f\"No trained vector found at {vector_path}\")\n",
    "        return None\n",
    "    \n",
    "    state = torch.load(vector_path, map_location=\"cpu\")\n",
    "    tensor = state.get(\"steering_vector\")\n",
    "    if tensor is None:\n",
    "        raise ValueError(f\"steering_vector.pt missing 'steering_vector' key at {vector_path}\")\n",
    "    \n",
    "    trained_vector = tensor.float()\n",
    "    trained_layer = TARGET_LAYER\n",
    "    \n",
    "    # Load layer from config if available\n",
    "    config_path = steering_dir / \"steering_config.json\"\n",
    "    if config_path.exists():\n",
    "        cfg = json.loads(config_path.read_text())\n",
    "        trained_layer = int(cfg.get(\"target_layer\", trained_layer))\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        trained_vector = trained_vector.cuda()\n",
    "    \n",
    "    norm = torch.linalg.norm(trained_vector).item()\n",
    "    print(f\"Loaded trained vector for {dataset}\")\n",
    "    print(f\"  Layer: {trained_layer}, Norm: {norm:.4f}, Shape: {trained_vector.shape}\")\n",
    "    \n",
    "    return trained_vector, trained_layer\n",
    "\n",
    "# Example usage\n",
    "# trained_vec, trained_layer = load_trained_steering_vector(\"qwen-3-32b__trait__acerbic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_activation_header",
   "metadata": {},
   "source": [
    "## 6. Load Activation (Vanilla) Steering Vector\n",
    "\n",
    "Load the activation-based steering vector (from CAA or similar methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "load_activation_vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_activation_steering_vector(\n",
    "    dataset: str,\n",
    "    layer: int = TARGET_LAYER,\n",
    "    persona_root: Path = PERSONA_ROOT,\n",
    ") -> torch.Tensor | None:\n",
    "    \"\"\"Load activation-based steering vector for a given dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset name (e.g., 'qwen-3-32b__trait__acerbic')\n",
    "        layer: Layer index to extract vector from\n",
    "        persona_root: Root directory containing activation vectors\n",
    "    \n",
    "    Returns:\n",
    "        Steering vector tensor or None if not found\n",
    "    \"\"\"\n",
    "    if \"__trait__\" in dataset:\n",
    "        model_prefix, trait = dataset.split(\"__trait__\", 1)\n",
    "        vec_file = persona_root / f\"{model_prefix}/traits_240/vectors/{trait}.pt\"\n",
    "        if not vec_file.exists():\n",
    "            print(f\"Trait vector not found: {vec_file}\")\n",
    "            return None\n",
    "        data = torch.load(vec_file, map_location=\"cpu\")\n",
    "        vec = data[\"pos_neg_50\"][layer].float()\n",
    "        \n",
    "    elif \"__role__\" in dataset:\n",
    "        role = dataset.split(\"__role__\", 1)[1]\n",
    "        vec_file = persona_root / f\"qwen-3-32b/roles_240/vectors/{role}.pt\"\n",
    "        if not vec_file.exists():\n",
    "            print(f\"Role vector not found: {vec_file}\")\n",
    "            return None\n",
    "        data = torch.load(vec_file, map_location=\"cpu\")\n",
    "        # Load default vectors for contrast\n",
    "        default_vecs = torch.load(\n",
    "            persona_root / \"qwen-3-32b/roles_240/default_vectors.pt\",\n",
    "            map_location=\"cpu\"\n",
    "        )\n",
    "        vec_pos = data[\"pos_3\"][layer].float()\n",
    "        vec_default = default_vecs[\"activations\"][\"default_1\"][layer].float()\n",
    "        vec = vec_pos - vec_default\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognized dataset format: {dataset}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        vec = vec.cuda()\n",
    "    \n",
    "    norm = torch.linalg.norm(vec).item()\n",
    "    print(f\"Loaded activation vector for {dataset}\")\n",
    "    print(f\"  Layer: {layer}, Norm: {norm:.4f}, Shape: {vec.shape}\")\n",
    "    \n",
    "    return vec\n",
    "\n",
    "# Example usage\n",
    "# activation_vec = load_activation_steering_vector(\"qwen-3-32b__trait__acerbic\", layer=TARGET_LAYER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructions_header",
   "metadata": {},
   "source": [
    "## 7. Load Instructions for Dataset\n",
    "\n",
    "Load prompts and questions from the instruction files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "load_instructions",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class InstructionData:\n",
    "    prompts: list[str]\n",
    "    questions: list[str]\n",
    "    eval_prompt: str | None\n",
    "\n",
    "def load_instructions(dataset: str) -> InstructionData:\n",
    "    \"\"\"Load instruction prompts and questions for a dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset name (e.g., 'qwen-3-32b__trait__acerbic')\n",
    "    \n",
    "    Returns:\n",
    "        InstructionData with prompts, questions, and optional eval_prompt\n",
    "    \"\"\"\n",
    "    instructions_root = Path.home() / \"persona-subspace\"\n",
    "    \n",
    "    if \"__trait__\" in dataset:\n",
    "        name = dataset.split(\"__trait__\", 1)[1]\n",
    "        path = instructions_root / \"traits\" / \"data\" / \"instructions\" / f\"{name}.json\"\n",
    "    elif \"__role__\" in dataset:\n",
    "        name = dataset.split(\"__role__\", 1)[1]\n",
    "        path = instructions_root / \"roles\" / \"data\" / \"instructions\" / f\"{name}.json\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognized dataset name: {dataset}\")\n",
    "\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Instructions not found: {path}\")\n",
    "\n",
    "    payload = json.loads(path.read_text())\n",
    "    prompts: list[str] = []\n",
    "    for entry in payload.get(\"instruction\", []):\n",
    "        if isinstance(entry, dict):\n",
    "            prompt = entry.get(\"pos\") or entry.get(\"prompt\")\n",
    "            if prompt:\n",
    "                prompts.append(prompt)\n",
    "        elif isinstance(entry, str):\n",
    "            prompts.append(entry)\n",
    "    if not prompts:\n",
    "        raise ValueError(f\"No prompts found in {path}\")\n",
    "    \n",
    "    questions = payload.get(\"questions\", [])\n",
    "    if not questions:\n",
    "        raise ValueError(f\"No questions in {path}\")\n",
    "    \n",
    "    eval_prompt = payload.get(\"eval_prompt\")\n",
    "    \n",
    "    print(f\"Loaded instructions for {dataset}\")\n",
    "    print(f\"  Prompts: {len(prompts)}, Questions: {len(questions)}\")\n",
    "    \n",
    "    return InstructionData(prompts=prompts, questions=questions, eval_prompt=eval_prompt)\n",
    "\n",
    "# Example usage\n",
    "# instructions = load_instructions(\"qwen-3-32b__trait__acerbic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generation_header",
   "metadata": {},
   "source": [
    "## 8. Generate Rollouts with Steering\n",
    "\n",
    "Generate text completions with or without steering vectors applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "generate_rollouts",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rollouts(\n",
    "    dataset: str,\n",
    "    instructions: InstructionData,\n",
    "    controller: SteeringController,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    vector: torch.Tensor | None = None,\n",
    "    layer: int = TARGET_LAYER,\n",
    "    variant_name: str = \"baseline\",\n",
    "    num_rollouts: int = 3,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    use_system_prompt: bool = True,\n",
    "    prompt_index: int = 0,\n",
    "    rescale_norm: float | None = None,\n",
    ") -> list[dict]:\n",
    "    \"\"\"Generate text completions with optional steering.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset name\n",
    "        instructions: InstructionData with prompts/questions\n",
    "        controller: SteeringController instance\n",
    "        tokenizer: Tokenizer instance\n",
    "        model: Model instance\n",
    "        vector: Steering vector to apply (None for baseline)\n",
    "        layer: Layer to apply steering\n",
    "        variant_name: Name for this variant (e.g., 'trained', 'activation', 'prompted')\n",
    "        num_rollouts: Number of rollouts per question\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature\n",
    "        top_p: Nucleus sampling parameter\n",
    "        use_system_prompt: Whether to include system prompt\n",
    "        prompt_index: Which prompt to use from instructions\n",
    "        rescale_norm: If provided, rescale vector to this L2 norm before applying\n",
    "    \n",
    "    Returns:\n",
    "        List of rollout dictionaries with metadata\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Rescale vector if requested\n",
    "    original_norm = None\n",
    "    if vector is not None and rescale_norm is not None:\n",
    "        original_norm = float(torch.linalg.norm(vector).item())\n",
    "        if original_norm > 0:\n",
    "            vector = vector * (rescale_norm / original_norm)\n",
    "            print(f\"Rescaled vector from {original_norm:.4f} to {torch.linalg.norm(vector).item():.4f}\")\n",
    "    \n",
    "    # Set up steering\n",
    "    controller.set_layer(layer)\n",
    "    controller.set_vector(vector)\n",
    "    \n",
    "    system_prompt = instructions.prompts[prompt_index] if instructions.prompts else None\n",
    "    \n",
    "    records = []\n",
    "    \n",
    "    # Disable thinking for Qwen models (prevents <think> blocks)\n",
    "    # See: https://github.com/vllm-project/vllm/issues/18066\n",
    "    chat_template_kwargs = {\"enable_thinking\": False} if \"qwen\" in BASE_MODEL.lower() else {}\n",
    "    \n",
    "    for rollout_idx in trange(num_rollouts, desc=f\"{variant_name}\", leave=False):\n",
    "        # Batch all questions for this rollout\n",
    "        messages_batch = []\n",
    "        for question in instructions.questions:\n",
    "            msgs = []\n",
    "            if use_system_prompt and system_prompt:\n",
    "                msgs.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "            msgs.append({\"role\": \"user\", \"content\": question})\n",
    "            messages_batch.append(msgs)\n",
    "        \n",
    "        # Apply chat template and tokenize (with thinking disabled for Qwen)\n",
    "        chat_texts = [\n",
    "            tokenizer.apply_chat_template(\n",
    "                msgs, \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=True,\n",
    "                **chat_template_kwargs\n",
    "            )\n",
    "            for msgs in messages_batch\n",
    "        ]\n",
    "        encoded = tokenizer(chat_texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        attention_mask = encoded.get(\"attention_mask\")\n",
    "        if attention_mask is None:\n",
    "            input_lens = torch.tensor([enc.size(0) for enc in encoded[\"input_ids\"]], device=device)\n",
    "        else:\n",
    "            input_lens = attention_mask.sum(dim=1)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **encoded,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "        \n",
    "        # Decode responses\n",
    "        for question_idx, question in enumerate(instructions.questions):\n",
    "            seq = outputs[question_idx]\n",
    "            offset = int(input_lens[question_idx])\n",
    "            response = tokenizer.decode(seq[offset:], skip_special_tokens=True).strip()\n",
    "            \n",
    "            record = {\n",
    "                \"dataset\": dataset,\n",
    "                \"variant\": variant_name,\n",
    "                \"prompt_index\": prompt_index if variant_name == \"prompted\" else None,\n",
    "                \"question_index\": question_idx,\n",
    "                \"rollout_index\": rollout_idx,\n",
    "                \"question\": question,\n",
    "                \"system_prompt\": system_prompt if use_system_prompt else None,\n",
    "                \"response\": response,\n",
    "                \"layer\": layer,\n",
    "            }\n",
    "            \n",
    "            if vector is not None:\n",
    "                final_norm = float(torch.linalg.norm(vector).item())\n",
    "                record[\"steering_norm\"] = final_norm\n",
    "                if original_norm is not None:\n",
    "                    record[\"steering_norm_original\"] = original_norm\n",
    "                if rescale_norm is not None:\n",
    "                    record[\"steering_norm_target\"] = rescale_norm\n",
    "            \n",
    "            records.append(record)\n",
    "    \n",
    "    print(f\"Generated {len(records)} rollouts for {variant_name}\")\n",
    "    return records\n",
    "\n",
    "# Example usage (commented out)\n",
    "# records = generate_rollouts(\n",
    "#     dataset=\"qwen-3-32b__trait__acerbic\",\n",
    "#     instructions=instructions,\n",
    "#     controller=controller,\n",
    "#     tokenizer=tokenizer,\n",
    "#     model=base_model,\n",
    "#     vector=trained_vec,\n",
    "#     layer=trained_layer,\n",
    "#     variant_name=\"trained\",\n",
    "#     num_rollouts=3,\n",
    "#     rescale_norm=10.0,  # Rescale to L2 norm of 10.0\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_rollouts_header",
   "metadata": {},
   "source": [
    "## 9. Save Rollouts to JSONL\n",
    "\n",
    "Save rollouts with metadata to a JSONL file for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "save_rollouts",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_rollouts(\n",
    "    records: list[dict],\n",
    "    dataset: str,\n",
    "    output_root: Path = ROLLOUT_OUTPUT_ROOT,\n",
    "    filename: str = \"rollouts.jsonl\",\n",
    ") -> Path:\n",
    "    \"\"\"Save rollout records to JSONL file.\n",
    "    \n",
    "    Args:\n",
    "        records: List of rollout dictionaries\n",
    "        dataset: Dataset name\n",
    "        output_root: Root output directory\n",
    "        filename: Output filename\n",
    "    \n",
    "    Returns:\n",
    "        Path to saved file\n",
    "    \"\"\"\n",
    "    output_dir = output_root / dataset\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_path = output_dir / filename\n",
    "    \n",
    "    with output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for record in records:\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    print(f\"Saved {len(records)} records to {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "# Example usage\n",
    "# save_rollouts(records, dataset=\"qwen-3-32b__trait__acerbic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete_pipeline_header",
   "metadata": {},
   "source": [
    "## 10. Complete Evaluation Pipeline\n",
    "\n",
    "Put it all together: evaluate a dataset with trained, activation, and prompted variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "complete_pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset_complete(\n",
    "    dataset: str,\n",
    "    num_rollouts: int = 3,\n",
    "    evaluate_trained: bool = True,\n",
    "    evaluate_activation: bool = True,\n",
    "    evaluate_prompted: bool = True,\n",
    "    evaluate_unprompted: bool = True,\n",
    "    save_outputs: bool = True,\n",
    "    trained_rescale_norm: float | None = None,\n",
    "    activation_rescale_norm: float | None = None,\n",
    ") -> dict[str, list[dict]]:\n",
    "    \"\"\"Complete evaluation pipeline for a dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset name (e.g., 'qwen-3-32b__trait__acerbic')\n",
    "        num_rollouts: Number of rollouts per question\n",
    "        evaluate_trained: Whether to evaluate trained steering vector\n",
    "        evaluate_activation: Whether to evaluate activation vector\n",
    "        evaluate_prompted: Whether to evaluate prompted baseline\n",
    "        evaluate_unprompted: Whether to evaluate unprompted control (no system prompt, no steering)\n",
    "        save_outputs: Whether to save rollouts to disk\n",
    "        trained_rescale_norm: Rescale trained vectors to this L2 norm (None = no rescaling)\n",
    "        activation_rescale_norm: Rescale activation vectors to this L2 norm (None = no rescaling)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping variant names to rollout records\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating: {dataset}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Load instructions\n",
    "    instructions = load_instructions(dataset)\n",
    "    \n",
    "    all_records = {}\n",
    "    \n",
    "    # Evaluate unprompted control (no system prompt, no steering)\n",
    "    if evaluate_unprompted:\n",
    "        print(\"\\n--- Unprompted Control (No System Prompt, No Steering) ---\")\n",
    "        unprompted_records = generate_rollouts(\n",
    "            dataset=dataset,\n",
    "            instructions=instructions,\n",
    "            controller=controller,\n",
    "            tokenizer=tokenizer,\n",
    "            model=base_model,\n",
    "            vector=None,\n",
    "            variant_name=\"unprompted\",\n",
    "            num_rollouts=num_rollouts,\n",
    "            use_system_prompt=False,\n",
    "        )\n",
    "        all_records[\"unprompted\"] = unprompted_records\n",
    "    \n",
    "    # Evaluate prompted baseline\n",
    "    if evaluate_prompted and instructions.prompts:\n",
    "        print(\"\\n--- Prompted Baseline ---\")\n",
    "        for prompt_idx in range(len(instructions.prompts)):\n",
    "            prompted_records = generate_rollouts(\n",
    "                dataset=dataset,\n",
    "                instructions=instructions,\n",
    "                controller=controller,\n",
    "                tokenizer=tokenizer,\n",
    "                model=base_model,\n",
    "                vector=None,\n",
    "                variant_name=\"prompted\",\n",
    "                num_rollouts=num_rollouts,\n",
    "                use_system_prompt=True,\n",
    "                prompt_index=prompt_idx,\n",
    "            )\n",
    "            all_records[f\"prompted_{prompt_idx}\"] = prompted_records\n",
    "    \n",
    "    # Evaluate trained vector\n",
    "    if evaluate_trained:\n",
    "        print(\"\\n--- Trained Steering Vector ---\")\n",
    "        result = load_trained_steering_vector(dataset)\n",
    "        if result is not None:\n",
    "            trained_vec, trained_layer = result\n",
    "            trained_records = generate_rollouts(\n",
    "                dataset=dataset,\n",
    "                instructions=instructions,\n",
    "                controller=controller,\n",
    "                tokenizer=tokenizer,\n",
    "                model=base_model,\n",
    "                vector=trained_vec,\n",
    "                layer=trained_layer,\n",
    "                variant_name=\"trained\",\n",
    "                num_rollouts=num_rollouts,\n",
    "                use_system_prompt=False,\n",
    "                rescale_norm=trained_rescale_norm,\n",
    "            )\n",
    "            all_records[\"trained\"] = trained_records\n",
    "    \n",
    "    # Evaluate activation vector\n",
    "    if evaluate_activation:\n",
    "        print(\"\\n--- Activation Steering Vector ---\")\n",
    "        activation_vec = load_activation_steering_vector(dataset, layer=TARGET_LAYER)\n",
    "        if activation_vec is not None:\n",
    "            activation_records = generate_rollouts(\n",
    "                dataset=dataset,\n",
    "                instructions=instructions,\n",
    "                controller=controller,\n",
    "                tokenizer=tokenizer,\n",
    "                model=base_model,\n",
    "                vector=activation_vec,\n",
    "                layer=TARGET_LAYER,\n",
    "                variant_name=\"activation\",\n",
    "                num_rollouts=num_rollouts,\n",
    "                use_system_prompt=False,\n",
    "                rescale_norm=activation_rescale_norm,\n",
    "            )\n",
    "            all_records[\"activation\"] = activation_records\n",
    "    \n",
    "    # Save all records\n",
    "    if save_outputs:\n",
    "        all_records_flat = []\n",
    "        for variant_records in all_records.values():\n",
    "            all_records_flat.extend(variant_records)\n",
    "        save_rollouts(all_records_flat, dataset=dataset)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluation complete: {sum(len(r) for r in all_records.values())} total rollouts\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return all_records\n",
    "\n",
    "# Example usage\n",
    "# results = evaluate_dataset_complete(\n",
    "#     dataset=\"qwen-3-32b__trait__acerbic\",\n",
    "#     num_rollouts=3,\n",
    "#     evaluate_unprompted=True,\n",
    "#     trained_rescale_norm=10.0,  # Rescale trained vector to L2 norm of 10\n",
    "#     activation_rescale_norm=10.0,  # Rescale activation vector to L2 norm of 10\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example_header",
   "metadata": {},
   "source": [
    "## 11. Example: Complete Workflow\n",
    "\n",
    "Run a complete evaluation for a specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "example_workflow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Evaluating: qwen-3-32b__trait__acerbic\n",
      "============================================================\n",
      "\n",
      "Loaded instructions for qwen-3-32b__trait__acerbic\n",
      "  Prompts: 5, Questions: 40\n",
      "\n",
      "--- Activation Steering Vector ---\n",
      "Loaded activation vector for qwen-3-32b__trait__acerbic\n",
      "  Layer: 30, Norm: 70.2444, Shape: torch.Size([5120])\n",
      "Rescaled vector from 70.2444 to 200.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 40 rollouts for activation\n",
      "Saved 40 records to /workspace/steering_rollouts_qwen3_layer_30/qwen-3-32b__trait__acerbic/rollouts.jsonl\n",
      "\n",
      "============================================================\n",
      "Evaluation complete: 40 total rollouts\n",
      "============================================================\n",
      "\n",
      "\n",
      "ACTIVATION - Sample response:\n",
      "Question: What do you think about people who are always late to meetings?\n",
      "Response: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "People who are always late to meetings? Ugh. It's like they're giving you a middle finger in the form of a time management seminar. First of all, lateness is a declaration of war on...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Example: Evaluate a specific dataset\n",
    "DATASET_TO_EVALUATE = \"qwen-3-32b__trait__acerbic\"\n",
    "\n",
    "# Run complete evaluation\n",
    "results = evaluate_dataset_complete(\n",
    "    dataset=DATASET_TO_EVALUATE,\n",
    "    num_rollouts=1,\n",
    "    evaluate_trained=False,\n",
    "    evaluate_activation=True,\n",
    "    evaluate_prompted=False,\n",
    "    evaluate_unprompted=False,\n",
    "    activation_rescale_norm=200.0,\n",
    "    save_outputs=True,\n",
    ")\n",
    "\n",
    "# Display sample outputs\n",
    "for variant_name, records in results.items():\n",
    "    print(f\"\\n{variant_name.upper()} - Sample response:\")\n",
    "    if records:\n",
    "        sample = records[0]\n",
    "        print(f\"Question: {sample['question']}\")\n",
    "        print(f\"Response: {sample['response'][:200]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup_header",
   "metadata": {},
   "source": [
    "## 12. Cleanup\n",
    "\n",
    "Clean up resources when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cleanup",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Stop here",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mStop here\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Clean up steering controller\u001b[39;00m\n\u001b[32m      4\u001b[39m controller.close()\n",
      "\u001b[31mException\u001b[39m: Stop here"
     ]
    }
   ],
   "source": [
    "raise Exception(\"Stop here\")\n",
    "\n",
    "# Clean up steering controller\n",
    "controller.close()\n",
    "\n",
    "# Clear CUDA cache if needed\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA cache cleared\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
